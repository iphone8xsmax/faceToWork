[toc]



#### 1.redis延时推送

需要存储任务具体内容，还要存储任务的执行时间，使用Sorted Set。

我们可以**把任务的描述序列化成字符串**，放在**Sorted Set的value**中，然后把**任务的执行时间戳作为score**，利用Sorted Set天然的**排序**特性，**执行时刻越早的会排在越前面**。这样一来，我们只要开一个或多个定时线程，**每隔一段时间去查**一下这个Sorted Set中score小于或等于当前时间戳的元素（这可以通过**zrangebyscore命令**实现），然后再执行元素对应的任务即可。当然，**执行完任务后，还要将元素从Sorted Set中删除**，避免任务重复执行。**如果是多个线程去轮询这个Sorted Set，还要考虑并发问题**，假如说一个任务到期了，也被多个线程拿到了，这个时候必须保证只有一个线程能执行这个任务，这可以通过**zrem命令**来实现，**只有删除成功了，才能执行任务**，这样就能保证任务不被多个任务重复执行了。

#### 2.实现一个本地延迟推送的数据结构（堆）

利用小顶堆的特性，每次把时间最早的元素放在堆顶等待执行。

#### 3.Zset底层

Zset底层利用压缩链表和跳表实现。

满足以下两个条件的时候使用ziplist:

1. 保存的元素少于128个
2. 保存的所有元素大小都小于64字节

这两个数值是可以通过`redis.conf`的`zset-max-ziplist-entries` 和 `zset-max-ziplist-value`选项 进行修改。

**ziplist 编码**的有序集合对象使用压缩列表作为底层实现，每个集合元素使用**两个紧挨在一起的压缩列表节点来保存**，第一个节点保存**元素的成员**，第二个节点保存**元素的分值**。并且压缩列表内的集合元素按分值**从小到大的顺序进行排列**，小的放置在靠近表头的位置，大的放置在靠近表尾的位置。

**skiplist 编码**的有序集合对象使用 zset 结构作为底层实现，一个 zset 结构同时包含一个字典和一个跳表：

```cpp
typedef struct zset{
     //跳跃表
     zskiplist *zsl;
     //字典
     dict *dice;
} zset;
```

**字典的键保存元素的值**，**字典的值则保存元素的分值**；跳跃表节点的 **object** 属性保存元素的**成员**，跳跃表节点的 **score** 属性保存**元素的分值**。

这两种数据结构会**通过指针来共享相同元素的成员和分值**，所以不会产生重复成员和分值，造成内存的浪费。

其实有序集合单独使用字典或跳跃表其中一种数据结构都可以实现，但是这里使用两种数据结构**组合起来**，原因是假如我们单独使用字典，虽然能以 O(1) 的时间复杂度查找成员的分值，但是因为字典是以无序的方式来保存集合元素，所以**每次进行范围操作的时候都要进行排序**；假如我们单独使用跳跃表来实现，虽然能执行范围操作，**但是查找操作有 O(1)的复杂度变为了O(logN)**。因此**Redis使用了两种数据结构来共同实现有序集合。**

#### 4.堆的性质

对于**大顶堆**来说，所有**父节点的值要大于其子节点的值**；对于**小顶堆**来说，所有**父节点的值要小于其子节点的值**。而他们的**兄弟节点之间并无任何大小上的关联**。

堆总是一棵完全二叉树。

#### 5.cache，缓存淘汰算法

###### ① FIFO/LRU/LFU算法剔除

剔除算法通常用于缓存使用量**超过了预设最大值**的时候，如何对现有数据进行剔除。例如 Redis 使用 **maxmemory-policy** 参数配置剔除策略。这种方式**一致性最差**。

**FIFO 先进先出策略**

FIFO（First In First Out）：先进先出策略，在实时性的场景下，需要经常访问最新的数据，那么就可以使用 FIFO，使得最先进入的数据（最晚的数据）被淘汰。

**LRU 最近最久未使用策略**

LRU（Least Recently Used）：最近最久未使用策略，优先淘汰最久未使用的数据，也就是上次被访问时间距离现在最久的数据。该策略可以保证内存中的数据都是热点数据，也就是经常被访问的数据，从而保证缓存命中率。

**LFU 最不经常使用策略**

LFU（Least Frequently Used）：最不经常使用策略，优先淘汰一段时间内使用次数最少的数据。

###### ② 超时剔除

通过给缓存数据设置**超时时间**，让其在过期时间后**自动删除**，如 Redis 提供的 expire 命令。

###### ③ 主动更新

对于**高一致性要求**场景下，需要在真实数据更新后，**立即**更新缓存数据。

#### 6.redis的map怎么实现？让你来设计一个map，你会考虑哪些因素

用哈希表作为字典的底层实现。dictht是哈希表的定义：

```cpp
typedef struct dictht {
    // 哈希表节点指针数组（俗称桶，bucket）
    dictEntry **table;      
    // 指针数组的大小
    unsigned long size;     
    // 指针数组的长度掩码，用于计算索引值
    unsigned long sizemask; 
    // 哈希表现有的节点数量
    unsigned long used;     
} dictht;
```

table是一个数组，数组中的元素都是一个指向dictEntry结构的指针，每个**dictEntry**结构保存着一个键值对。

```cpp
typedef struct dictEntry {
    // 键
    void *key;
    // 值
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
    } v;
    // 链往后继节点
    struct dictEntry *next; 
} dictEntry;
```

可以看到有个next指针，是用链表法来解决hash冲突的；v保存值，可以是一个指针，uint64_t整数，或者int64_t整数。

Redis中字典的结构如下：

```cpp
typedef struct dict {
    // 特定于类型的处理函数
    dictType *type;
    // 类型处理函数的私有数据
    void *privdata;
    // 哈希表（2个）
    dictht ht[2];       
    // 记录 rehash 进度的标志，值为-1 表示 rehash 未进行
    int rehashidx;
    // 当前正在运作的安全迭代器数量
    int iterators;      
} dict;
```

这里需要解释一下dictType和privdata，前者是一组用于**操作键值对的函数**，redis会对不同用途的字典使用不同的函数，后者是这些函数需要用的可选参数。

ht[2]就是**两个哈希表**，一般情况下只会ht[0]，**ht[1]会在对ht[0]进行rehash时使用**。**rehashidx记录了rehash目前的进度**，如果目前没有进行rehash那么rehashidx=-1。

##### 设计map需要考虑的因素

HashMap 无非就是一个存储 <key,value> 格式的集合，使得通过 key 在 O(1) 的时间复杂下就能查找到 value。

基本原理就是**将 key 经过 hash 函数进行散列得到散列值**，然后**通过散列值对数组取模得到对应的 index** 。

所以 **hash 函数**很关键，不仅**运算要快，还需要分布均匀，减少 hash 碰撞**。

而因为输入值是无限的，而数组的大小是有限的所以肯定会有碰撞，因此可以采用**拉链法来处理冲突**。

为了避免恶意的 hash 攻击，**当拉链超过一定长度之后可以转为红黑树结构**。

当然**超过一定的结点还是需要扩容**的，不然碰撞就太严重了。

而普通的扩容会导致某次 put 延时较大，特别是 HashMap 存储的数据比较多的时候，所以可以考虑和 redis 那样搞**两个 table 延迟移动**，**一次可以只移动一部分**。还有，最好使用之前预估准数据大小，避免频繁的扩容。

还需要考虑多**线程安全**问题。

#### 7.了解哈希吗？

通过散列函数运算将任意长度的输入转换为固定长度的输出，即散列值，是一种压缩映射。

#### 8.哈希冲突解决？

##### 1.开放定址法

**一旦发生了冲突，就去寻找下一个空的散列地址**，只要散列表足够大，空的散列地址总能找到，并将记录存入 。

##### 2.再哈希法

再哈希法又叫双哈希法，**有多个不同的Hash函数**，当发生冲突时，使用第二个，第三个，….，等哈希函数
计算地址，直到无冲突。虽然不易发生聚集，但是增加了计算时间。

##### 3.链地址法

**每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来**。

##### 4.建立公共溢出区

将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。

#### 9.Redis底层数据结构

##### 数据类型

###### String

Redis 会根据当前值的**类型和长度**决定使用哪种内部编码实现。：

- int：8 字节的长整形。
- embstr：小于等于39 个字节的字符串。
- raw：大于 39 个字节的字符串。

典型使用场景：

- 缓存
- 计数
- 共享session
- 限制短期内访问次数

###### Hash

- **ziplist**（压缩列表）：当 filed 个数少且没有大的 value 时使用
- **hashtable**（哈希表）：当 filed 个数多或有大的 value 时使用

###### List

列表用于存储多个**有序**的字符串，每个字符串称为元素。由于元素是有序的，所以可以通过索引来访问元素或者进行范围访问。

可以充当栈与队列的角色。

内部编码:

- ziplist：压缩列表。元素少且元素长度小时使用，减少内存消耗。
- linkedlist：链表。元素多使用。

使用场景：

① **消息队列**

使用 lpush + **brpop** 可以实现阻塞队列，生产者客户端使用 lpush 生产，多个消费者客户端从使用 brpop 命令阻塞式“抢”列表中的元素，多个客户端保证了消费的负载均衡和高可用。

- **lpush + lpop = Stack**（栈）
- lpush + rpop = Queue（队列）
- lpush + ltrim = Capped Collection（有限集合）
- **lpush + brpop = Message Queue**（消息队列）

② **文章列表**

每个用户的文章列表用 list 存储，然后可以分页获取列表。

###### Set

用于保存多个字符串元素，与列表不同，set 中元素**不能重复**，并且集合中元素是**无序**的。除增删改查之外还可以做多个集合的交集、并集、差集。

- intset：元素是整数且个数较少时使用。
- hashtable：元素较多或不全是整数时使用。

**使用场景**

- 标签
- 生成随机数、抽奖
- 社交需求，计算共同兴趣

###### Zset

相当于在 **SET 类型**的基础上增加了一个**分数**，其特性为元素不可重复且多了**排序**的功能。元素不能重复但是 score 可以重复。

- ziplist：压缩列表。元素个数较少且值较小时用。
- skiplist：**跳跃表**。元素个数多或值大时用。

使用场景：排行榜系统

##### 底层数据结构

<img src="file://C:/Users/wbt/Desktop/faceToWork/Redis/%E5%9F%BA%E7%A1%80%E7%AF%87.assets/fb7e3612ddee8a0ea49b7c40673a0cf0.jpg?lastModify=1615340302" alt="img" style="zoom:50%;" />

###### 压缩列表

压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示**列表长度、列表尾的偏移量和列表中的 entry 个数**；压缩列表在表尾还有一个  zlend，表示列表结束。

在压缩列表中，如果我们**要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)**。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了。

###### 跳表

有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，**跳表在链表的基础上，增加了多级索引**，通过索引位置的几个跳转，实现数据的快速定位，如下图所示：

![img](file://C:\Users\wbt\Desktop\faceToWork\Redis\基础篇.assets\1eca7135d38de2yy16681c2bbc4f3fb4.jpg?lastModify=1615340493)



#### 10.Redis持久化策略，指令

redis的持久化方式有俩种，持久化策略有4种：

- RDB（数据快照模式），**定期存储**，保存的是**数据本身**，存储文件是紧凑的
- AOF（追加模式），**每次修改数据时，同步到硬盘(写操作日志)**，保存的是**数据的变更记录**
- 如果只希望数据保存在内存中的话，俩种策略都可以关闭
- 也可以同时开启俩种策略，**当Redis重启时，AOF文件会用于重建原始数据**

##### **RDB**

RDB定时备份内存中的数据集。服务器启动的时候，可以从 RDB 文件中恢复数据集。

###### 优点

- 存储的文件是紧凑的
- 适合用于**备份**，方便**恢复不同版本的数据**
- 适合于**容灾恢复**，备份文件可以在其他服务器恢复
- 最大化了Redis的性能，**备份的时候启动的是子线程，父进程不需要执行IO操作**
- 数据保存**比AOF要快**

###### 缺点

- 如果Redis因为**没有正确关闭**而停止工作是，**到上个保存点之间的数据将会丢失**
- 由于需要经常fork子线程来进行备份操作，如果**数据量很大的话，fork比较耗时**，如果cpu性能不够，服务器可能是卡顿。数据量大的时候，一个服务器不要部署多个Redis服务。

创建快照有以下5种形式：

- 客户端发送``BGSAVE``指令，服务端会fork一条子线程将快照写入磁盘
- 客户端发送SAVE指令，服务端**在主线程进行写入动作**。一般不常使用，一般在内存不够去执行BGSVAE的时候才用
- 设置了**SAVE配置项**，如SAVE 300 100，那么当“300秒内有100次写入”时，Redus会自动触发BGSAVE命令。如果有多个配置项，任意一个满足，都会触发备份
- Redis通过SHUTDOWN命令**接收到关闭服务器的请求**、或者TERM信号时，会**执行SAVE命令**，这时候**会阻塞所有客户端，不在执行客户端发送的任何命令**
- 当一个Redis服务器连接另外一个Redis服务器，并像对方发送**SYNC**命令开始一次复制操作时，如果主服务器目前没有在执行BGSAVE操作，或者主服务器刚刚执行完，那么主服务器就会执行BGSAVE

##### **AOF**(append only file)

AOF记录服务器的**所有写操作**。在服务器**重新启动的时候，会把所有的写操作重新执行一遍**，从而实现数据备份。当写操作集过大（比原有的数据集还大），Redis 会**重写写操作集**。

在后台重建AOF文件，而不会影响client端操作。在任何时候执行BGREWRITEAOF命令，都会把当前内存中最短序列的命令写到磁盘，这些命令可以完全构建当前的数据情况，而不会存在多余的变化情况（比如状态变化，计数器变化等），缩小的AOF文件的大小。所以当使用AOF时，redis推荐同时使用**BGREWRITEAOF**。

AOF文件刷新的方式，有三种，参考配置参数appendfsync ：appendfsync always**每提交一个修改命令都调用fsync刷新到AOF文件**，非常非常慢，但也非常安全；appendfsync everysec**每秒钟都调用fsync刷新到AOF文件，很快，但可能会丢失一秒以内的数据**；appendfsync no**依靠OS进行刷新，redis不主动刷新AOF，这样最快，但安全性就差**。默认并**推荐每秒刷新**，这样在速度和安全上都做到了兼顾。

**LOG Rewrite的工作原理**:同样用到了copy-on-write：首先redis会fork一个子进程；子进程将最新的AOF写入一个临时文件；父进程增量的把内存中的最新执行的修改写入（这时仍写入旧的AOF，rewrite如果失败也是安全的）；当子进程完成rewrite临时文件后，父进程会收到一个信号，并把之前内存中增量的修改写入临时文件末尾；这时redis将旧AOF文件重命名，临时文件重命名，开始向新的AOF中写入。

###### 优点

- 使用AOF模式更加的灵活，因为可以有不同的fsync策略
- AOF是一个**日志追加文件**，所有不需要定位，就算**断电也没有损坏问题，哪怕文件末尾是一个写到一半的命令，redus-check-aof工具也可以很轻易的修复**
- 当AOF文件很大的，Redis会自动在后台进行**重写**。重写是**绝对安全**的，因为Redis是**继续往旧的文件里面追加**，使用**创建当前数据集所需的最小操作集合来创建一个全新的文件**，一旦创建完成，Redis就会**切换到新文件，开始往新文件进行追加操作**
- AOF包含一个又一个的操作命令，易于理解和解析

###### 缺点

- 对于同样的数据集，**AOF文件通常要大于RDB文件**
- **AOF可能比RDB要慢**，这取决于fsync策略。通常fsync设置为每秒一次的话性能仍然很高，如果关闭sfync，即使在很高的负载下也和RDB一样快。不过，**即使在很大的写负载情况下，RDB还是能提供很好的最大延迟保证**
- AOF通过**递增**的方式更新数据，而RDB快照是**从头开始**创建，RDB会更健壮和稳定（所以适用于备份）

#### 11.为什么使用单线程？

##### 1.多线程的开销

通常情况下，在我们采用多线程后，如果没有良好的系统设计，刚开始增加线程数时，系统吞吐率会增加，但是，再进一步增加线程时，系统吞吐率就增长迟缓了，有时甚至还会出现下降的情况。瓶颈在于，**系统中通常会存在被多线程同时访问的共享资源，比如一个共享的数据结构**。当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个额外的机制，就会带来**额外的开销**。

##### 2.单线程快

- 在内存存储， 读写速度快；

- 高效的数据结构，如哈希表，跳表等；

- IO非阻塞，多路复用机制

    **该机制允许内核中，同时存在多个监听套接字和已连接套接字**。内核会一直监听这些套接字上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。Redis 网络框架调用 **epoll 机制**，让内核监听这些套接字。此时，Redis  线程不会阻塞在某一个特定的监听或已连接套接字上，也就是说，不会阻塞在某一个特定的客户端请求处理上。正因为此，Redis  可以同时和多个客户端连接并处理请求，从而提升并发性。

    为了在请求到达时能通知到 Redis 线程，**select/epoll 提供了基于事件的回调机制**，即针对不同事件的发生，调用相应的处理函数。

    select/epoll **一旦监测到 FD 上有请求到达时，就会触发相应的事件**。这些事件会被放进一个**事件队列**，Redis  单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时，Redis  在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为 Redis  一直在对事件队列进行处理，所以能及时响应客户端请求，提升 Redis 的响应性能。


#### 12.go map 和 redis map

![Redis map类型结构](../Untitled.assets/20191212154259174.png)

<img src="../Untitled.assets/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vNzY5ODA4OC81NzU3Njk4Ni1hY2Q4NzYwMC03NDlmLTExZTktODcxMC03NWU0MjNjN2VmZGIucG5n" alt="Go map类型结构" style="zoom: 33%;" />

##### 数据结构

###### 相同

内部两个哈希表，用于扩容，但Go中叫做**buckets和oldbuckets**，**Redis中是一个数组**，大小为2

###### 不同

- 层次不同。 参见上面的图，Redis第二层存储了子表的信息，第三层作为子表，存储的是实际数据的地址；Go实际只有三层。这导致两种实现后续功能的差异，如：**是否支持缩容**。
- size、used存储方式。Go在顶层结构中存储了B字段，表示有2^B个bucket，并存储了count字段表示已有数据个数；Redis在第二层(每个字表信息)中
    k-v排列方式。Go：8*key+8*val作为一个bucket，后面可以链式挂接更多overflow的bucket，Redis：key+val作为一个dictEntity，后面可以链式挂接更多dictEntity

##### 哈希方式

相同：根据不同类型，调用不同的hash方法后，求余得到索引
不同：Redis：得到bucket索引后即得到bucket数据，二Go还需要再根据tophash->key的查找
冲突解决方式
相同：拉链法
不同：Redis的链表直接存在每个数据(dictEntity)后，Go由8个k-v组成一个bucket，然后再挂接overflow bucket

##### rehash

相同：装载因子的概念
相同：渐进式扩容 下文详细描述
不同：触发扩容的时机 Go在插入新key时检测装载因子和拉链长度，Redis在增删查改时都会检查是否需要rehash
不同：触发扩容的条件 Go：bucket内(可以存储8个k-v)的平均个数超过6.5或单个bucket的overflow超过bucket数会进行扩容；Redis：每个key平均存储了一个数据，则进行扩容；每个key平均存储了不到0.1个数据，则进行缩容
思考：

假如Go的数据主要集中在一个bucket里，其overflow很长，其他bucket的数据很少，这时应该触发rehash吗？
这要分两种情况：

如果该bucket包括overflow中的数据量比较满，那么map整体的数据量也接近6.6/bucket了，rehash扩容后，hash后的key被重新打散，数据会被重新分配到其他bucket中；

如果该overflow中的位置比较空，rehash后bucket数量不变，该bucket中的数据被重新依次填入到新的bucket中，空位消失，overflow数量也就减少了

##### 渐进式扩容

相同：第一次先分配空间，后面再渐进搬迁
不同：Go只在增删操作时搬迁，Redis在增删查改操作时都会进行渐进搬迁操作

##### 缩容

不同：Go只保存了当前的bucket size，新bucket一定是旧bucket大小的两倍，不支持缩容。Redis的缩容：两个子表记录了自己的大小，缩容即扩容的逆过程

##### 代码组织方式

不同：Go编译器会做很多trick的处理，比如给bmap填充真实的字段和数据，而Redis使用C编写，代码一目了然

#### 13.缓存和数据库一致性的问题

缓存和 DB 的一致性，我们**指的更多的是最终一致性**。我们**使用缓存只要是提高读操作的性能，真正在写操作的业务逻辑，还是以数据库为准。**

##### 更新缓存的设计模式

主要有两种情况，会导致缓存和 DB 的一致性问题：

1. 并发的场景下，导致**读取老的 DB 数据，更新到缓存中**。
2. 缓存和 DB 的操作，**不在一个事务中**，可能只有一个操作成功，而另一个操作失败，导致不一致。

###### 1.旁路缓存 Cache Aside Pattern

- **失效**：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。

- **命中**：应用程序从cache中取数据，取到后返回。

- **更新**：**先把数据存到数据库中**，成功后，**再让缓存失效**。

一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。

###### 2.Read/Write Through Pattern

把更新数据库（Repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。**可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的Cache。**

Read Through 套路就是**在查询操作中更新缓存**，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside是由调用方负责把数据加载入缓存，而Read Through则**用缓存服务自己来加载**，从而对应用方是透明的。

Write Through 套路和Read Through相仿，**不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由Cache自己更新数据库（这是一个同步操作）**

###### 3.Write Behind Caching Pattern

Write Behind 又叫 Write Back。**write back就是Linux文件系统的Page Cache的算法**。

在更新数据的时候，**只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库**。

> 这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。

但是，其带来的问题是，**数据不是强一致性的，而且可能会丢失**（我们知道Unix/Linux非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍Trade-Off。

另外，Write Back实现逻辑比较复杂，因为他需要**track有哪数据是被更新了的**，需要刷到持久层上。操作系统的**write back会在仅当这个cache需要失效的时候，才会被真正持久起来**，比如，内存不够了，或是进程退出了等情况，这又叫lazy write。

##### 缓存架构设计

###### 1.更新缓存 VS 淘汰缓存

**更新缓存**：数据不但写入数据库，还会写入缓存；优点：**缓存不会增加一次miss，命中率高**

**淘汰缓存**：数据只会写入数据库，不会写入缓存，**只会把数据淘汰掉**；优点：简单

当更新缓存的代价很大的时候，**淘汰缓存操作简单，并且带来的副作用只是增加了一次cache miss，建议作为通用的处理方式。**

###### 2.先操作数据库 vs 先操作缓存

当写操作发生时，假设淘汰缓存作为对缓存通用的处理方式，又面临两种抉择：

1. 先写数据库，再淘汰缓存
2. 先淘汰缓存，再写数据库

假设先写数据库，再淘汰缓存：第一步写数据库操作成功，第二步淘汰缓存失败，则会出现DB中是新数据，Cache中是旧数据，数据不一致。假设先淘汰缓存，再写数据库：第一步淘汰缓存成功，第二步写数据库失败，则只会引发一次Cache miss。

**结论：数据和缓存的操作时序：先淘汰缓存，再写数据库。**

###### 3.缓存架构优化

一种方案是服务化：加入一个服务层，向上游提供帅气的数据访问接口，**向上游屏蔽底层数据存储的细节**，这样业务线不需要关注数据是来自于cache还是DB。

另一种方案是**异步缓存更新**：业务线所有的写操作都走数据库，所有的读操作都走缓存，由一个异步的工具来做数据库与缓存之间数据的同步，具体细节是：

1. 要有一个init cache的过程，将需要缓存的数据**全量写入**cache
2. 如果DB有写操作，异步更新程序读取binlog，更新cache

在1和2的合作下，cache中有全部的数据，这样：

（a）业务线读cache，一定能够hit（**很短的时间内，可能有脏数据**），无需关注数据库

（b）业务线写DB，cache中能得到异步更新，无需关注缓存

这样将大大简化业务线的调用逻辑，存在的缺点是，如果缓存的数据业务逻辑比较复杂，async-update异步更新的逻辑可能也会比较复杂。

##### 缓存和DB一致性的解决方案

###### 先淘汰缓存，再写数据库

因为先淘汰缓存，所以数据的最终一致性是可以得到有效的保证的。因为先淘汰缓存，**即使写数据库发生异常，也就是下次缓存读取时，多读取一次数据库**。

但是，这种方案会存在缓存和 DB 的数据会不一致的情况（对同一个数据进行读写，在数据库层面**并发的读写并不能保证完成顺序**，也就是说后发出的读请求很可能先完成（读出脏数据）），我们**需要解决缓存并行写，实现串行写**。比较简单的方式，引入**分布式锁**。

- 在写请求时，先淘汰缓存之前，获取该分布式锁。
- 在读请求时，发现缓存不存在时，先获取分布式锁。

这样，缓存的并行写就成功的变成串行写落。写请求时，是否主动更新缓存，根据自己业务的需要，是否有，都没问题。

###### 先写数据库，再更新缓存

按照“先写数据库，再更新缓存”，我们要保证 **DB 和缓存的操作，能够在“同一个事务”中**，从而实现最终一致性。

**基于定时任务来实现**

- 首先，写入数据库。
- 然后，在写入数据库所在的事务中，插入一条记录到**任务表**。该记录会存储**需要更新的缓存 KEY 和 VALUE** 。
- 【异步】最后，定时任务每秒扫描任务表，更新到缓存中，之后删除该记录。

**基于消息队列来实现**

- 首先，写入数据库。
- 然后，**发送带有缓存 KEY 和 VALUE 的事务消息**。此时，需要有支持事务消息特性的消息队列，或者我们自己封装消息队列，支持事务消息。
- 【异步】最后，消费者消费该消息，更新到缓存中。

这两种方式，可以进一步优化，**可以先尝试更新缓存，如果失败，则插入任务表，或者事务消息。**

**如果网络抖动，导致【插入任务表，或者事务消息】的顺序不一致？**

那么怎么解决呢？需要做如下三件事情：

1. 在缓存值中，**拼接上数据版本号或者时间戳**。例如说：value = {value: 原值, version: xxx} 。

2. 在**任务表的记录，或者事务消息中，增加上数据版本号或者时间戳的字段**。

3. 在定时任务或消息队列执行更新缓存时，**先读取缓存，对比版本号或时间戳，大于才进行更新**。 当然，此处也会有并发问题，所以还是得引入**分布式锁或 CAS 操作**。

###### 基于数据库的 binlog 日志

**1.重客户端**

#### 14.redis和etcd的区别

- 从数据结构方面来讲 Redis支持多种数据类型（string，set，list，hash，zset） 
- 从读写性能上来讲，Redis读写性能优异，并且提供了RDB、AOF持久化，而etcd v3的底层采用boltdb做存储，value直接持久化 
- 从使用场景上来看，etcd更适用于服务发现，配置管理，而Redis更适用于非强一致性的需求，比如说是队列，缓存，分布式Session
- 两者都是KV存储，但是etcd通过Raft算法保证了各个节点间的数据和事务的一致性，更强调各个节点间的通信；Redis则时更像是内存式的缓存，因此来说读写能力很强。 
- Redis是c开发的，etcd是go开发的，他是源于k8s的兴起作为一个服务发现。 
- etcd v3只能通过gRPC访问，而redis可以通过http访问，因此etcd的客户端开发工作量高很多。

#### 15.etcd的raft算法

#### 16.Redis缓存策略

参考上面的缓存与数据库一致性问题的设计。

#### 17.分布式CAP原理

Consistency（一致性）即更新操作成功并返回客户端后，**所有节点在同一时间的数据完全一致**，这就是分布式的一致性。一致性的问题在并发系统中不可避免，对于客户端来说，一致性指的是并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。

Availability（可用性）即**服务一直可用，而且是正常响应时间**。好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。

Partition tolerance（分区容错性）分布式系统**在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务**。

##### CAP定理

要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。也就是说分布式系统不可能同时满足三个特性。

##### 取舍策略

**CA without P**：如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是**分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的**。

**CP without A**：如果不要求A（可用），相当于**每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)**，一旦发生网络故障或者消息丢失等情况，就要**牺牲用户的体验**，等待所有数据全部一致了之后再让用户访问系统。设计成CP的系统其实不少，最典型的就是**分布式数据库，如Redis、HBase**等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。

 **AP wihtout C**：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致**全局数据的不一致性**。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。

#### 18.Redis的多集群方案

Redis支持三种集群方案[https://www.cnblogs.com/spec-dog/p/12501895.html]

##### 主从复制模式

主从复制模式中包含一个主数据库实例（master）与一个或多个从数据库实例（slave），如下图

![redis-master-slave](../Untitled.assets/632381-20200316092434553-1122086987.png)

客户端可对主数据库进行读写操作，对从数据库进行读操作，主数据库写入的数据会实时自动同步给从数据库。

具体**工作机制**为：

1. slave启动后，**向master发送SYNC命令**，master接收到SYNC命令后通过**bgsave保存快照**，并使用**缓冲区记录保存快照这段时间内执行的写命令**
2. master**将保存的快照文件发送给slave**，并**继续记录执行的写命令**
3. slave接收到快照文件后，**加载快照文件，载入数据**
4. master快照发送完后开始向slave发送**缓冲区的写命令**，slave接收命令并执行，完成复制初始化
5. 此后master**每次执行一个写命令都会同步发送给slave**，保持master与slave之间数据的一致性

**优点：**

1. master能自动将数据同步到slave，可以进行**读写分离，分担master的读压力**
2. master、slave之间的**同步是以非阻塞的方式进行的**，同步期间，客户端仍然可以提交查询或更新请求

**缺点：**

1. 不具备**自动容错与恢复功能**，master或slave的**宕机**都可能导致客户端请求失败，需要等待机器重启或手动切换客户端IP才能恢复
2. master宕机，如果**宕机前数据没有同步完，则切换IP后会存在数据不一致**的问题
3. 难以支持在线扩容，**Redis的容量受限于单机配置**

##### Sentinel（哨兵）模式

哨兵模式基于主从复制模式，只是引入了哨兵来监控与自动处理故障。如图

![redis-sentinel](../Untitled.assets/632381-20200316092434904-227928571.png)

哨兵顾名思义，就是来为Redis集群站哨的，一旦发现问题能做出相应的应对处理。其功能包括

1. 监控master、slave是否正常运行
2. 当master出现故障时，能自动将一个slave转换为master（大哥挂了，选一个小弟上位）
3. 多个哨兵可以监控同一个Redis，哨兵之间也会自动监控

哨兵模式的具体工作机制：

在配置文件中通过 `sentinel monitor <master-name> <ip> <redis-port> <quorum>` 来定位master的IP、端口，一个哨兵可以监控多个master数据库，只需要提供多个该配置项即可。哨兵启动后，**会与要监控的master建立两条连接**：

1. 一条连接用来**订阅master的`_sentinel_:hello`频道与获取其他监控该master的哨兵节点信息**
2. 另一条连接**定期向master发送INFO等命令获取master本身的信息**

与master建立连接后，哨兵会执行三个操作：

1. 定期（一般10s一次，当master被标记为主观下线时，改为1s一次）**向master和slave发送INFO命令**
2. 定期**向master和slave的`_sentinel_:hello`频道发送自己的信息**
3. 定期（1s一次）**向master、slave和其他哨兵发送PING命令**

发送INFO命令可以**获取当前数据库的相关信息从而实现新节点的自动发现**。所以说哨兵只需要配置master数据库信息就可以自动发现其slave信息。获取到slave信息后，**哨兵也会与slave建立两条连接执行监控**。通过INFO命令，哨兵可以获取主从数据库的最新信息，并进行相应的操作，比如角色变更等。

接下来哨兵向主从数据库的_sentinel_:hello频道发送信息**与同样监控这些数据库的哨兵共享自己的信息**，发送内容为哨兵的ip端口、运行id、配置版本、master名字、master的ip端口还有master的配置版本。这些信息有以下用处：

1. 其他哨兵可以通过该信息**判断发送者是否是新发现的哨兵**，如果是的话会创建一个到该哨兵的连接用于发送PING命令。
2. 其他哨兵通过该信息可以**判断master的版本，如果该版本高于直接记录的版本，将会更新**
3. 当实现了自动发现slave和其他哨兵节点后，哨兵就可以通过定期发送PING命令定时监控这些数据库和节点有没有停止服务。

如果被PING的数据库或者节点超时（通过 `sentinel down-after-milliseconds master-name milliseconds` 配置）未回复，哨兵认为其**主观下线**（sdown，s就是Subjectively —— 主观地）。如果下线的是master，哨兵会向其它哨兵发送命令询问它们是否也认为该master主观下线，如果**达到一定数目**（即配置文件中的quorum）投票，哨兵会认为该master已经**客观下线**（odown，o就是Objectively —— 客观地），并**选举领头的哨兵节点对主从系统发起故障恢复**。若没有足够的sentinel进程同意master下线，master的客观下线状态会被移除，若master重新向sentinel进程发送的PING命令返回有效回复，master的主观下线状态就会被移除.

哨兵认为master客观下线后，故障恢复的操作需要由选举的领头哨兵来执行，选举采用**Raft算法**：

1. **发现master下线的哨兵节点**（我们称他为A）向每个哨兵发送命令，要求对方选自己为领头哨兵
2. 如果目标哨兵节点**没有选过其他人**，则会同意选举A为领头哨兵
3. 如果有**超过一半**的哨兵同意选举A为领头，则A当选
4. 如果有多个哨兵节点同时参选领头，此时有可能存在一轮投票无竞选者胜出，此时每个参选的节点**等待一个随机时间后**再次发起参选请求，进行下一轮投票竞选，直至选举出领头哨兵

选出领头哨兵后，领头者开始对系统进行故障恢复，从出现故障的master的**从数据库中挑选一个来当选新的master**,选择规则如下：

1. 所有在线的slave中选择**优先级最高**的，优先级可以通过slave-priority配置
2. 如果有多个最高优先级的slave，则选取**复制偏移量最大（即复制越完整）**的当选
3. 如果以上条件都一样，选取**id最小**的slave

挑选出需要继任的slave后，领头哨兵向该数据库发送命令**使其升格为master**，然后再**向其他slave发送命令接受新的master**，最后更新数据。**将已经停止的旧的master更新为新的master的从数据库，使其恢复服务后以slave的身份继续运行**。

**优点：**

1. 哨兵模式基于主从复制模式，所以主从复制模式有的优点，哨兵模式也有
2. 哨兵模式下，**master挂掉可以自动进行切换**，系统可用性更高

**缺点：**

1. 同样也继承了主从模式难以在线扩容的缺点，Redis的容量受限于单机配置
2. 需要额外的资源来启动sentinel进程，实现相对复杂一点，同时slave节点作为备份节点不提供服务

##### Cluster模式

**哨兵模式解决了主从复制不能自动故障转移，达不到高可用的问题，但还是存在难以在线扩容，Redis容量受限于单机配置的问题。****Cluster模式实现了Redis的分布式存储，即每台节点存储不同的内容，来解决在线扩容的问题。**如图

![redis-cluster](../Untitled.assets/632381-20200316092435392-508884462.png)



Cluster采用**无中心结构**,它的特点如下：

1. 所有的**redis节点彼此互联**(PING-PONG机制),**内部使用二进制协议优化传输速度和带宽**
2. 节点的**fail是通过集群中超过半数的节点检测失效时才生效**
3. 客户端与redis节点直连,**不需要中间代理层**.客户端不需要连接集群所有节点,**连接集群中任何一个可用节点**即可

Cluster模式的具体工作机制：

1. 在Redis的每个节点上，都有一个**插槽**（slot），取值范围为**0-16383**
2. 当我们存取key的时候，Redis会根据**CRC16的算法**得出一个结果，然后**把结果对16384求余数**，这样**每个key都会对应一个编号在0-16383之间的哈希槽**，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作
3. 为了保证高可用，Cluster模式也**引入主从复制模式**，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点
4. **当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时**，那么认为主节点A宕机了。如果主节点A和它的从节点都宕机了，那么该集群就无法再提供服务了

Cluster模式集群节点**最小配置6个节点(3主3从，因为需要半数以上)**，其中主节点提供读写操作，**从节点作为备用节点，不提供请求，只作为故障转移使用**。

**优点：**

1. 无中心架构，数据按照slot分布在多个节点。
2. **集群中的每个节点都是平等的关系，每个节点都保存各自的数据和整个集群的状态**。每个节点都和其他所有节点连接，而且这些连接保持活跃，这样就保证了我们只需要连接集群中的任意一个节点，就可以获取到其他节点的数据。
3. **可线性扩展到1000多个节点**，节点可动态添加或删除。
4. 能够实现**自动故障转移**，节点之间通过gossip协议交换状态信息，用投票机制完成slave到master的角色转换。

**缺点：**

1. 客户端实现复杂，驱动要求实现Smart Client，缓存slots mapping信息并及时更新，提高了**开发难度**。目前仅JedisCluster相对成熟，异常处理还不完善，比如常见的“max redirect exception”
2. 节点会因为某些原因发生**阻塞**（阻塞时间大于 cluster-node-timeout）被判断下线，这种failover是没有必要的
3. 数据通过**异步复制，不保证数据的强一致性**
4. slave充当“**冷备**”，**不能缓解读压力**
5. **批量操作限制，目前只支持具有相同slot值的key执行批量操作，对mset、mget、sunion等操作支持不友好**
6. key事务操作支持有限，**只支持多key在同一节点的事务操作，多key分布不同节点时无法使用事务功能**
7. 不支持多数据库空间，**单机redis可以支持16个db，集群模式下只能使用一个**，即db 0

Redis Cluster模式**不建议使用pipeline和multi-keys操作，减少max redirect产生的场景**。

#### 19.Redis哨兵如何通信，哨兵如何监控数据节点，哨兵自己失效如何处理？

send hello - sentinelSendHello（向Sentinel/Master/Slave 发送 publish 命令，**通过共同订阅同一个Master的消息队列**（sentinel hello频道）使得多个Sentinel 可以相互连接），正常频率为每 2s 执行一次。

1）每个Sentinel以每秒钟一次的频率向它所知的**Master，Slave以及其他 Sentinel** 实例发送一个**PING命令**。
2）如果一个实例（instance）距离最后一次有效回复PING命令的时间**超过 own-after-milliseconds 选项所指定的值**，则这个实例会被Sentinel标记为**主观下线**。 
3）如果一个Master被标记为主观下线，则正在监视这个Master的**所有 Sentinel 要以每秒一次的频率确认Master**的确进入了主观下线状态。 
4）当有足够数量的Sentinel（大于等于配置文件指定的值）在指定的时间范围内确认Master的确进入了主观下线状态，则Master会被标记为**客观下线**。
5）在一般情况下，每个Sentinel 会以每10秒一次的频率向它已知的**所有Master，Slave发送 INFO 命令**。
6）当Master被Sentinel标记为**客观下线**时，Sentinel 向下线的 Master 的所有**Slave**发送 **INFO命令的频率会从10秒一次改为每秒一次**。 
7）若没有足够数量的Sentinel同意Master已经下线，Master的客观下线状态就会被移除。 若 Master重新向Sentinel 的PING命令返回有效回复，Master的主观下线状态就会被移除。

重启。

#### 20.LRU算法中，如果有数据插入进来会到队列头部，那如果有大量数据插入进来，原来的热数据是不是会被挤掉？怎么解决这个问题？

**因为只看数据的访问时间，使用 LRU 策略在处理扫描式单次查询操作时，无法解决缓存污染**。为了应对这类缓存污染问题，Redis 从 4.0 版本开始增加了 LFU 淘汰策略。与 LRU 策略相比，LFU 策略中会从两个维度来筛选并淘汰数据：一是，数据访问的**时效性**（访问时间离当前时间的远近）；二是，数据的**被访问次数**

在 LRU 策略基础上，为每个数据增加了一个**计数器，来统计这个数据的访问次数**。当使用 LFU 策略筛选淘汰数据时，**首先会根据数据的访问次数进行筛选**，把访问次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU 策略**再比较这两个数据的访问时效性**，把距离上一次访问时间更久的数据淘汰出缓存。

为了避免操作链表的开销，Redis 在实现 LRU 策略时使用了两个近似方法：

- Redis 是用 **RedisObject 结构**来保存数据的，RedisObject 结构中设置了一个 **lru 字段，用来记录数据的访问时间戳**；
- Redis 并没有为所有的数据维护一个全局的链表，而是通过**随机采样**方式，选取一定数量（例如 10 个）的数据放入候选集合，后续在候选集合中根据 lru 字段值的大小进行筛选。

在此基础上，**Redis 在实现 LFU 策略的时候，只是把原来 24bit 大小的 lru 字段，又进一步拆分成了两部分**。

1. ldt 值：lru 字段的前 16bit，表示数据的访问时间戳；
2. counter 值：lru 字段的后 8bit，表示数据的访问次数。

在实际应用中，一个数据可能会被访问成千上万次。如果每被访问一次，counter 值就加 1 的话，那么，只要访问次数超过了 255，数据的 counter 值就一样了。在进行数据淘汰时，LFU 策略就无法很好地区分并筛选这些数据，反而还可能会把不怎么访问的数据留存在了缓存中。LFU 策略实现的**计数规则**是：每当数据被访问一次时，首先，用计数器当前的值乘以**配置项 lfu_log_factor** 再加 1，再取其倒数，得到一个 p 值；然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，**只有 p 值大于 r 值时，计数器才加 1**。

#### 21.一致性哈希

Distributed Hash Table（DHT） 是一种**哈希分布方式**，其目的是为了克服传统哈希分布在服务器节点数量变化再哈希时**大量数据迁移**的问题。

##### 1. 基本原理

将哈希空间 [0, 2<sup>n</sup>-1] 看成一个**哈希环**，每个服务器**节点**都配置到**哈希环**上。**==每个数据对象通过哈希取模得到哈希值之后，存放到哈希环中顺时针方向第一个大于等于该哈希值的节点上==**。一致性哈希在增加或者删除节点时只会影响到哈希环中相邻的节点。

##### 2. 虚拟节点

上面描述的一致性哈希存在数据**分布不均匀**的问题，节点存储的数据量有可能会存在**很大的不同**。

数据不均匀主要是因为**节点在哈希环上分布的不均匀**，这种情况在**节点数量很少**的情况下尤其明显。

解决方式是通过**增加虚拟节点**，然后将**虚拟节点映射到真实节点**上。虚拟节点的数量比真实节点来得**多**，那么虚拟节点在哈希环上分布的均匀性就会比原来的真实节点好，从而使得数据分布也更加均匀。

#### 22.Redis的内存淘汰策略

![img](file://C:/Users/wbt/Desktop/faceToWork/Redis/%E5%AE%9E%E8%B7%B5%E7%AF%87.assets/04bdd13b760016ec3b30f4b02e133df6.jpg?lastModify=1615255247)

**默认**情况下，Redis 在使用的内存空间超过 maxmemory 值时，并**不会淘汰数据**，也就是设定的 **noeviction 策略**。对应到 Redis 缓存，也就是指，一旦缓存被写满了，再有写请求来时，Redis 不再提供服务，而是直接返回错误。Redis 用作缓存时，实际的数据集通常都是大于缓存容量的，总会有新的数据要写入缓存，这个策略本身不淘汰数据，也就不会腾出新的缓存空间，我们不把它用在 Redis 缓存中。

 `volatile-random、volatile-ttl、volatile-lru 和 volatile-lfu` 这四种淘汰策略。它们筛选的候选数据范围，被限制在已经设置了过期时间的键值对上。也正因为此，**即使缓存没有写满，这些数据如果过期了，也会被删除**。

- volatile-ttl 在筛选时，会针对设置了过期时间的键值对，**根据过期时间的先后进行删除**，越早过期的越先被删除。
- volatile-random 就像它的名称一样，在设置了过期时间的键值对中，进行**随机删除**。
- volatile-lru 会使用 LRU 算法筛选设置了过期时间的键值对。
- volatile-lfu 会使用 LFU 算法选择设置了过期时间的键值对。

`allkeys-lru、allkeys-random、allkeys-lfu `这三种淘汰策略的备选淘汰数据范围，就扩大到了所有键值对，**无论这些键值对是否设置了过期时间**。它们筛选数据进行淘汰的规则是：

- allkeys-random 策略，从所有键值对中**随机选择并删除数据**；
- allkeys-lru 策略，使用 LRU 算法在所有数据中进行筛选。
- allkeys-lfu 策略，使用 LFU 算法在所有数据中进行筛选。

LRU 算法在实际实现时，需要用链表管理所有的缓存数据，这会**带来额外的空间开销**。而且，当有数据被访问时，需要在链表上把该数据移动到 MRU 端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。

在 Redis 中，LRU 算法被做了简化，以减轻数据淘汰对缓存性能的影响。具体来说，Redis 默认会记录每个数据的最近一次访问的时间戳（由键值对数据结构 RedisObject 中的 lru 字段记录）。然后，Redis 在决定淘汰的数据时，**第一次会随机选出 N 个数据，把它们作为一个候选集合**。接下来，Redis 会**比较这 N 个数据的 lru 字段，把 lru 字段值最小的数据从缓存中淘汰出去**。Redis 提供了一个配置参数 `maxmemory-samples`，这个参数就是 Redis 选出的数据个数 N。

当需要再次淘汰数据时，Redis 需要挑选数据**进入第一次淘汰时创建的候选集合**。这儿的挑选标准是：**能进入候选集合的数据的 lru 字段值必须小于候选集合中最小的 lru 值**。当有新数据进入候选数据集后，如果候选数据集中的数据个数达到了 maxmemory-samples，Redis 就把候选数据集中 lru 字段值最小的数据淘汰出去。这样一来，Redis 缓存不用为所有的数据维护一个大链表，也不用在每次数据访问时都移动链表项，提升了缓存的性能。

- 优先使用 **allkeys-lru** 策略。这样，可以充分利用 LRU 这一经典缓存算法的优势，把最近最常访问的数据留在缓存中，提升应用的访问性能。如果你的业务数据中**有明显的冷热数据区分**，我建议你使用 allkeys-lru 策略。
- 如果业务应用中的**数据访问频率相差不大，没有明显的冷热数据区分**，建议使用 **allkeys-random** 策略，随机选择淘汰的数据就行。
- 如果你的**业务中有置顶的需求**，比如置顶新闻、置顶视频，那么，可以使用 **volatile-lru** 策略，**同时不给这些置顶数据设置过期时间**。这样一来，这些需要置顶的数据一直不会被删除，而其他数据会在过期时根据 LRU 规则进行筛选。

##### 如何处理被淘汰的数据？

一般来说，一旦被淘汰的数据选定后，如果这个数据是干净数据，那么我们就直接删除；如果这个数据是脏数据，我们需要把它写回数据库。干净数据和脏数据的区别就在于，**和最初从后端数据库里读取时的值相比，有没有被修改过**。干净数据一直没有被修改，所以后端数据库里的数据也是最新值。在替换时，它可以被直接删除。

不过，对于 Redis 来说，它决定了被淘汰的数据后，**会把它们删除**。即使淘汰的数据是脏数据，Redis 也不会把它们写回数据库。所以，我们在使用 Redis 缓存时，如果数据被修改了，需要在数据修改时就将它写回数据库。否则，这个脏数据被淘汰时，会被 Redis 删除，而数据库里也没有最新的数据了。

#### 23.Redis读性能问题怎么解决？

##### Redis 真的变慢了吗？

- **查看 Redis 的响应延迟**。当你发现  Redis 命令的执行时间突然就增长到了几秒，基本就可以认定 Redis 变慢了。这种方法是看 Redis  延迟的绝对值，但是，**在不同的软硬件环境下，Redis 本身的绝对性能并不相同**。
- **基于当前环境下的 Redis 基线性能做判断**。基线性能就是一个系统**在低压力、无干扰下的基本性能**，这个性能只由当前的软硬件配置决定。redis-cli 命令提供了**–intrinsic-latency 选项，可以用来监测和统计测试期间内的最大延迟**，这个延迟可以作为  Redis 的基线性能。其中，测试时长可以用–intrinsic-latency  选项的参数来指定。

##### 如何应对 Redis 变慢？

**Redis 自身的操作特性、文件系统和操作系统**，它们是影响 Redis 性能的三大要素。

![img](file://C:/Users/wbt/Desktop/faceToWork/Redis/%E5%AE%9E%E8%B7%B5%E7%AF%87.assets/cd026801924e197f5c79828c368cd706.jpg?lastModify=1615258483)

##### Redis 自身操作特性的影响

###### 1. 慢查询命令

当你发现 Redis 性能变慢时，可以通过 Redis 日志，或者是 latency monitor 工具，查询变慢的请求，根据请求对应的具体命令以及官方文档，确认下是否采用了复杂度高的慢查询命令。

如果的确有大量的慢查询命令，有两种处理方式：

- **用其他高效命令代替**。比如说，如果你需要返回一个  SET 中的所有成员时，不要使用 SMEMBERS 命令，而是要使用 SSCAN  多次迭代返回，避免一次返回大量数据，造成线程阻塞。
- **当你需要执行排序、交集、并集操作时，可以在客户端完成，而不要用  SORT、SUNION、SINTER 这些命令，以免拖慢 Redis 实例。**

###### 2. 过期 key 操作

Redis 键值对的 key 可以设置过期时间。默认情况下，Redis 每 100 毫秒会删除一些过期 key，具体的算法如下：

- 采样 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 个数的 key，并将其中过期的 key 全部删除；
- **如果超过 25% 的 key 过期了，则重复删除的过程，直到过期 key 的比例降至 25% 以下**。

如果触发了上面这个算法的第二条，Redis 就会一直删除以释放内存空间。注意，**删除操作是阻塞的**（**Redis 4.0 后可以用异步线程机制来减少阻塞影响**）。所以，一旦该条件触发，Redis 的线程就会一直执行删除，这样一来，就没办法正常服务其他的键值操作了，就会进一步引起其他键值操作的延迟增加，Redis 就会变慢。所以要尽量**避免同一时间大量key过期**。

要根据实际业务的使用需求，决定 EXPIREAT 和 EXPIRE  的过期时间参数。其次，如果一批 key 的确是同时过期，你还可以**在 EXPIREAT 和 EXPIRE  的过期时间参数上，加上一个一定大小范围内的随机数**，这样，既保证了 key 在一个邻近时间范围内被删除，又避免了同时过期造成的压力。

##### 文件系统的影响：AOF模式

为了保证数据可靠性，Redis 会采用 AOF 日志或 RDB  快照。其中，AOF 日志提供了三种日志写回策略：no、everysec、always。这三种**写回策略依赖文件系统的两个系统调用完成**，也就是  **write 和 fsync**。

write 只要把日志记录写到内核缓冲区，就可以返回了，并不需要等待日志实际写回到磁盘；而 fsync 需要把日志记录写回到磁盘后才能返回，时间较长。下面这张表展示了三种写回策略所执行的系统调用。

![img](file://C:/Users/wbt/Desktop/faceToWork/Redis/%E5%AE%9E%E8%B7%B5%E7%AF%87.assets/9f1316094001ca64c8dfca37c2c49ea4.jpg?lastModify=1615258483)

当写回策略配置为 everysec 和  always 时，Redis 需要调用 fsync 把日志写回磁盘。当写回策略配置为 **everysec 时，Redis  会使用后台的子线程异步完成 fsync 的操作**。而对于 **always** 策略来说，Redis  需要确保每个操作记录日志都写回磁盘，如果用后台子线程异步完成，主线程就无法及时地知道每个操作是否已经完成了，这就不符合 always  策略的要求了。所以，always 策略并**不使用后台子线程**来执行。fsync  的执行时间很长，如果是在 Redis 主线程中执行 fsync，就容易阻塞主线程。

Redis  使用子进程来进行 AOF 重写。AOF 重写会对磁盘进行大量 IO 操作，同时，fsync 又需要等到数据写到磁盘后才能返回，所以，**当 AOF 重写的压力比较大时，就会导致 fsync 被阻塞**。虽然 fsync 是由后台子线程负责执行的，但是，主线程会监控 fsync 的执行进度。如果的确需要高性能，同时也需要高可靠数据保证，我建议你考虑**采用高速的固态硬盘作为 AOF 日志的写入设备**。

##### 操作系统的影响

###### swap

正常情况下，Redis 的操作是直接通过访问内存就能完成，一旦  swap 被触发了，Redis 的请求操作需要等到磁盘数据读写完成才行。而且，和我刚才说的 AOF 日志文件读写使用 fsync  线程不同，**swap 触发后影响的是 Redis 主 IO 线程，这会极大地增加 Redis  的响应时间**。

通常，触发  swap 的原因主要是**物理机器内存不足**，对于 Redis 而言，有两种常见的情况：

- Redis  实例**自身使用了大量的内存**，导致物理机器的可用内存不足；
- 和 Redis  实例在同一台机器上运行的其他进程，在进行大量的文件读写操作。文件读写本身会占用系统内存，这会导致**分配给 Redis 实例的内存量变少，进而触发  Redis 发生 swap**。

针对这个问题，我也给你提供一个解决思路：**增加机器的内存或者使用 Redis 集群**。当出现百 MB，甚至 GB 级别的 swap 大小时，就表明，此时，Redis 实例的内存压力很大，很有可能会变慢。所以，swap 的大小是排查 Redis 性能变慢是否由 swap 引起的重要指标。

###### 内存大页

Linux 内核从 2.6.38 开始支持内存大页机制，该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB  的粒度来执行的。

虽然内存大页可以给 Redis  带来**内存分配方面的收益**，但是，Redis  为了提供数据可靠性保证，需要**将数据做持久化保存**。这个写入过程由额外的线程执行，此时，Redis  主线程仍然可以接收客户端写请求。**客户端的写请求可能会修改正在进行持久化的数据**。在这一过程中，Redis  就会采用**写时复制机制**，也就是说，一旦有数据要被修改，Redis  **并不会直接修改内存中的数据，而是将这些数据拷贝一份，然后再进行修改**。如果采用了内存大页，那么，即使客户端请求只修改 100B 的数据，Redis 也需要拷贝 2MB 的大页。**当客户端请求修改或新写入数据较多时，内存大页机制将导致大量的拷贝，这就会影响 Redis  正常的访存操作，最终导致性能变慢。**

#### 24.Redis中如何使用epoll模型的？如果存在延时任务如何做？

在 Redis  只运行单线程的情况下，**该机制允许内核中，同时存在多个监听套接字和已连接套接字**。内核会一直监听这些套接字上的**连接请求或数据请求**。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。

![img](file://C:\Users\wbt\Desktop\faceToWork\Redis\基础篇.assets\00ff790d4f6225aaeeebba34a71d8bea.jpg?lastModify=1615260666)

为了在请求到达时能通知到 Redis 线程，**select/epoll 提供了基于事件的回调机制**，即针对不同事件的发生，调用相应的处理函数。

那么，回调机制是怎么工作的呢？其实，select/epoll 一旦监测到 FD 上有请求到达时，就会触发相应的事件。

这些事件会被放进一个**事件队列**，Redis  单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时，**Redis  在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调**。因为 Redis  一直在对事件队列进行处理，所以能及时响应客户端请求，提升 Redis 的响应性能。

#### 25.Redis主从复制

作用：数据冗余、故障恢复、负载均衡、高可用基石

##### 复制原理

复制大致有六个过程：

① **保存主节点信息**。

② **连接主节点**。从节点内部通过每秒运行的**定时任务**维护复制相关逻辑，当定时任务**发现存在新的主节点**的时候，会尝试与该节点**建立连接**。连接成功会产生日志，连接失败则会无限重连。

③ **发送 ping 命令**。从节点发送 ping 指令给主节点进行**首次通信**，用于检测网络情况和主节点是否可以处理请求。如果返回 pong 失败，则主节点会**断开连接**下一次定时任务继续**重连**。

④ **权限认证**。这发生在主节点设置了 requirepass 参数时。

⑤ **数据同步**。主从连接首次通信正常后，主节点会把持有的数据**全部发送**给从节点，这是**非常耗时**的操作。

⑥ **命令持续复制**。接下来主节点会**持续**把命令发送给从节点，保证主从数据一致性。

##### psync

psync 命令用于实现**数据同步**。**psync 运行需要的核心组件如下**：

- 主从节点各自**复制偏移量**。

- 主节点复制积压缓冲区。

    是保存在**主节点**上的一个**固定长度的队列**，默认 1MB，当主节点在于从节点连接后创建。这时候主节点写命令的时候，**不但把命令发给从节点，还会写入到复制积压缓冲区。**是一个**定长的先进先出队列**（FIFO），所以能够实现**保存最近已复制数据**的功能，用于**部分复制和复制命令丢失时的数据补救**。

- 主节点运行 id。每个Redis节点启动后都会动态分配一个 40 位的十六进制字符串作为运行 Id，可以唯一标识 Redis 节点。

从节点使用 **psync** 命令完成部分**复制和全量复制**功能。

<img src="file://C:\Users\wbt\Desktop\faceToWork\缓存\assets\image-20200426105809787.png?lastModify=1615289774" alt="image-20200426105809787" style="zoom: 67%;" />

主节点根据 psync 参数与自身参数情况选择进行回复。

- +**FULLRESYNC** {runId} {offset}：从节点触发**全量复制**。
- +**CONTINUE**：从节点触发部分复制。
- +**ERR**：主节点版本低于2.8，无法识别 psync 命令。

Redis 2.8 之后使用 **psync** 命令完成主从数据同步，同步可以分为：**全量复制、部分复制**。

- **全量复制**：一般用于**初次复制**的场景，开销较大。主从第一次建立复制时必须经历的阶段，触发全量复制的命令是 **sync（Redis2.8之前） 和 fsync（Redis2.8之后）**。
    - 由于是第一次复制，从节点没有复制偏移量和主节点运行 Id，所以发送 **fsync ？-1**。
    - 主节点判断这是全量复制，返回 **+FULLRESYNC**。
    - 从节点接收到主节点的响应数据，即可**保存运行 Id 和偏移量 offset**。
    - 主节点执行 **bgsave** 命令保存 **RDB 文件到本地**。(耗时)
    - 主节点把 RDB 文件**发送**到从节点，从节点将其作为本地数据文件。如果 RDB 文件过大，那么发送过程可能非常耗时。(耗时)
    - 从节点从开始接收 RDB 快照到接收完成期间，主节点仍然可以响应指令，主节点把这一时期的命令数据保存在**复制客户端缓冲区**内，等从节点加载完 RDB 文件后，主节点**再把缓冲区内的数据发送到从节点**，保证数据一致性。
    - 从节点接收完主节点传送来的数据后会**清空自身旧数据**，然后**加载 RDB 文件**。(耗时)
    - 如果从节点开启了 AOF 持久化，那么加载完 RDB 后会立即执行 **bgrewriteaof** 操作进行持久化。(耗时)
- **部分复制**：用于处理在主从复制中因**网络闪断**等原因造成的**数据丢失**场景，从节点再次连接主节点后，如果条件允许，主节点**补发**丢失的数据给从节点。可以避免开销过大。
    - 主从节点如果出现网络中断，时间超过 repl-timeout 时就认为从节点故障并中断复制链接。
    - 此时主节点依然可以响应指令，但是复制链接中断所以**无法**发送命令给从节点，不过主节点内部**存在复制积压缓冲区**，依然**可以保存最近一段时间**内的写命令数据。
    - 当主从再次连接之后，两者通过**偏移量 offset** 在**复制积压缓冲区**寻找，如果 offset 在缓冲区中，则**直接根据这个偏移量补发**数据即可。

全量复制发生场景：第一次建立复制、节点运行ID不匹配、复制挤压缓冲区不足(offset不在积压缓冲区中)

##### 心跳机制

主从结点在建立复制之后，它们之间维护着**长连接**并彼此发送**心跳命令**。

心跳机制：

- 主从节点都有心跳检查机制。
- 主节点默认每隔 10 秒对从节点发送 ping 命令，判断从节点的状态。
- 从节点在主线程中每隔 1 秒发送 replconf ack {offset} 命令给主节点报告自己的 offset 信息。

##### 异步复制

主节点不但负责数据读写，而且还会把写命令同步给从节点。**写命令的发送**过程是**异步**完成的，也就是说主节点自身处理完读写命令后**直接返回给客户端**，并**不等待从节点复制**完成。**由于主从复制是异步的，所以会造成从节点的数据相对主节点存在==延迟==。**具体延迟可以通过 **info replication** 中**主从节点的复制偏移量**计算出。

#### 26.redis读写大文件阻塞怎么处理？

#### 27.redis缓存雪崩、缓存穿透和缓存击穿

##### 缓存雪崩

指**大量的应用请求无法在 Redis 缓存中进行处理**，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。一般是由两个原因导致的。

第一个原因是：**缓存中有大量数据同时过期**，**导致大量请求无法得到处理**。

解决方案：

- **避免给大量的数据设置相同的过期时间**。（实在不行加一个随机数）
- **服务降级**，来应对缓存雪崩。是指发生缓存雪崩时，针对不同的数据采取不同的处理方式。
    - 当业务应用访问的是**非核心数据**（例如电商商品属性）时，暂时停止从缓存中查询这些数据，而是**直接返回预定义信息、空值或是错误信息**；
    - 当业务应用访问的是**核心数据**（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。

第二个原因是：**Redis 缓存实例发生故障宕机了，无法处理请求，这就会导致大量请求一下子积压到数据库层，从而发生缓存雪崩。**

解决方案：

- 第一个建议，是**在业务系统中实现服务熔断或请求限流机制。**
- 第二个建议就是**事前预防**。

##### 缓存击穿

**针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理**，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。缓存击穿的情况，经常发生在热点数据过期失效时。

解决方案：**对于访问特别频繁的热点数据，我们就不设置过期时间了**。

##### 缓存穿透

指要访问的数据既不在 Redis 缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。此时，应用也无法从数据库中读取数据再写入缓存，来服务后续请求，这样一来，缓存也就成了“摆设”，如果应用持续有大量请求访问数据，就会同时给缓存和数据库带来巨大压力。

发生情况：

- 业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据；
- 恶意攻击：专门访问数据库中没有的数据。

解决方案：

- **缓存空值或缺省值**

    一旦发生缓存穿透，我们就可以针对查询的数据，在 Redis 中**缓存**一个空值或是和业务层协商确定的**缺省值**。紧接着，应用发送的后续请求再进行查询时，就可以**直接从 Redis 中读取空值或缺省值，返回给业务应用了**，避免了把大量请求发送给数据库处理，保持了数据库的正常运行。

- **使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力**

    布隆过滤器由一个初值都为 0 的 bit 数组和 N 个哈希函数组成，可以用来快速判断某个数据是否存在。

- 在请求入口的**前端进行请求检测**。