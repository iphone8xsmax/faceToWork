# 1. 基础
## 1.1 系统性能评价
### 1.1.1 性能指标
#### 1. 响应时间
指某个请求从**发出到接收到响应**消耗的时间。在对响应时间进行测试时，通常采用重复请求的方式，然后计算平均响应时间。
#### 2. 吞吐量
指系统在**单位时间**内可以处理的**请求数量**，通常使用**每秒的请求数**来衡量。
#### 3. 并发用户数
指系统能**同时处理的并发用户请求**数量。在没有并发存在的系统中，请求被**顺序执行**，此时**响应时间为吞吐量的倒数**。例如系统支持的吞吐量为 100 req/s，那么平均响应时间应该为 0.01s。

目前的大型系统都支持多线程来处理并发请求，多线程能够提高吞吐量以及缩短响应时间，主要有两个原因：
- 多 CPU
- IO 等待时间

使用 **IO 多路复用** 等方式，系统在等待一个 IO 操作完成的这段时间内不需要被阻塞，可以去处理其它请求。通过将这个等待时间利用起来，使得 **CPU 利用率** 大大提高。并发用户数不是越高越好，因为如果并发用户数太高，系统来不及处理这么多的请求，会使得过多的请求需要等待，那么响应时间就会大大提高。

### 1.1.2 性能优化
#### 1. 集群
将多台服务器组成**集群**，使用**负载均衡**将请求转发到集群中，避免单一服务器的负载压力过大导致性能降低。
#### 2. 缓存
**缓存** 能够提高性能的原因如下：
- 缓存数据通常位于**内存等介质**中，这种介质对于**读操作**特别快；
- 缓存数据可以位于**靠近用户的地理位置**上；
- 可以将计算结果进行缓存，从而**避免重复计算**。

#### 3. 异步
某些流程可以将操作转换为消息，将消息发送到 **消息队列** 之后立即返回，之后这个操作会被 **异步处理** 。

## 1.2 系统特性
### 1.2.1 伸缩性
指不断向集群中**添加服务器**来缓解不断上升的用户**并发访问压力和不断增长的数据存储需求**。
#### 1. 伸缩性与性能
如果系统存在**性能问题**，那么单个用户的请求总是很慢的；如果系统存在**伸缩性问题**，那么**单个**用户的**请求可能会很快**，但是在**并发数很高**的情况下系统会**很慢**。
#### 2. 实现伸缩性
应用服务器只要**不具有状态**，那么就可以很容易地通过负载均衡器向集群中添加新的服务器。

**关系型数据库** 的伸缩性通过 **Sharding** 来实现，将**数据按一定的规则分布到不同的节点**上，从而解决单台存储服务器的存储空间限制。对于**非关系型数据库**，它们天生就是为海量数据而诞生，对伸缩性的支持特别好。
### 1.2.2 扩展性
指的是添加**新功能**时对现有系统的其它应用无影响，这就要求不同应用具备**低耦合**的特点。

实现可扩展主要有两种方式：
- 使用**消息队列**进行解耦，应用之间通过消息传递进行通信；
- 使用**分布式服务**将业务和可复用的服务分离开来，业务使用分布式服务框架调用可复用的服务。新增的产品可以通过调用可复用的服务来实现业务逻辑，对其它产品没有影响。

### 1.2.3 可用性
#### 1. 冗余
保证**高可用**的主要手段是使用**冗余**，当某个服务器故障时就请求其它服务器。

**应用服务器** 的冗余比较容易实现，只要 **保证应用服务器不具有状态**，那么某个应用服务器故障时，负载均衡器将该应用服务器原先的用户请求转发到另一个应用服务器上，不会对用户有任何影响。

**存储服务器** 的冗余需要使用 **主从复制** 来实现，当主服务器故障时，需要提升从服务器为主服务器，这个过程称为 **切换**。

#### 2. 监控
对 CPU、内存、磁盘、网络等系统负载信息进行监控，当某个信息达到一定阈值时通知运维人员，从而在系统发生故障之前及时发现问题。
#### 3. 服务降级
**服务降级** 是系统为了应对大量的请求，**主动关闭部分功能**，从而 **保证核心功能** 可用。

### 1.2.4 安全性
要求系统在应对各种攻击手段时能够有可靠的应对措施。

## 1.3 CAP理论
分布式系统不可能同时满足**一致性**（C：Consistency）、**可用性**（A：Availability）和**分区容忍性**（P：Partition Tolerance），最多只能同时满足**其中两项**。
<img src="assets/image-20200529113206691.png" alt="image-20200529113206691" style="zoom:42%;" />

### 1.3.1 一致性Consistency
**一致性** 指的是 **多个数据副本** 是否能 **保持一致** 的特性，在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。

对系统的一个数据**更新成功**之后，如果所有用户都能够**读取到最新的值**，该系统就被认为具有**强一致性**。

### 1.3.2 可用性Availability
**可用性** 指分布式系统在面对 **各种异常** 时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。

在可用性条件下，要求系统提供的**服务一直处于可用的状态**，对于用户的每一个操作请求总是能够在有限的时间内返回结果。

### 1.3.3 分区容忍性Partion Tolerance
**网络分区** 指分布式系统中的节点被 **划分为多个区域**，每个区域**内部可以通信**，但是区域之间无法通信。

在分区容忍性条件下，分布式系统在**遇到任何网络分区故障**的时候，**仍然需要能对外提供一致性和可用性的服务**，除非是整个网络环境都发生了故障。

### 1.3.4 权衡
在分布式系统中，分区容忍性**必不可少**，因为需要总是假设网络是不可靠的。因此，CAP 理论实际上是要在**可用性和一致性**之间做权衡。

**当发生网络分区（P）的时候，如果要继续服务，那么强一致性和可用性只能 2 选 1。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）是必须要实现的**。

在多个节点之间进行数据同步时：
- 为了保证一致性（**CP**），不能访问未同步完成的节点，也就失去了部分可用性。
- 为了保证可用性（**AP**），允许读取所有节点的数据，但是数据可能不一致。

## 1.4 BASE理论
BASE 是**基本可用**（Basically Available）、**软状态**（Soft State）和**最终一致性**（Eventually Consistent）三个短语的缩写。

BASE 理论是对 CAP 中**一致性和可用性权衡**的结果，它的核心思想是：**即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性**。

也就是牺牲数据的**一致性**来满足系统的**高可用性**，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。

### 1.4.1 基本可用
指分布式系统在出现故障的时候，保证**核心可用**，允许**损失部分可用性**。例如，电商在做促销时，为了保证**购物系统的稳定性**，部分消费者可能会被引导到一个**降级的页面**。

### 1.4.2 软状态
**软状态** 指允许系统中的数据存在 **中间状态**，并认为该中间状态**不会影响系统整体可用性**，即允许系统不同节点的**数据副本之间进行同步的过程存在一定时延**。

### 1.4.3 最终一致性
**最终一致性** 强调的是系统中所有的数据副本，在经过一段时间的同步后，**最终能达到一致的状态**。

**ACID** 要求**强一致性**，通常运用在**传统的数据库系统**上。而 **BASE** 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在**大型分布式系统**中。在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会**结合在一起使用**。

### 1.4.4 应用
针对**数据库**领域，BASE 思想的主要实现是对**业务数据进行拆分**，让不同的数据分布在不同的机器上，以提升系统的可用性，当前主要有以下两种做法：
- 按**功能划分**数据库。
- **分片**（如开源的 Mycat、Amoeba 等）。

由于拆分后会涉及**分布式事务**问题，所以 eBay 在该 BASE 论文中提到了如何用最终一致性的思路来实现高性能的分布式事务。

## 1.5 负载均衡
集群中的应用服务器（节点）通常被设计成无状态，用户可以请求任何一个节点。

负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上。

负载均衡器可以用来实现高可用以及伸缩性：
- 高可用：当某个节点故障时，负载均衡器会将用户请求转发到另外的节点上，从而保证所有服务持续可用；
- 伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点。

负载均衡器运行过程包含两个部分：
1. 根据负载均衡算法得到转发的节点；
2. 进行转发。

### 1.5.1 负载均衡算法
#### 1. 轮询算法
**轮询算法把每个请求轮流发送到每个服务器上**。

该算法比较适合每个服务器的**性能差不多**的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载。

#### 2. 加权轮询算法
加权轮询是在轮询的基础上，根据服务器的性能差异，为**服务器赋予一定的权值**，性能高的服务器分配更高的权值。

#### 3. 最少连接算法
由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成**负载不均衡**。

**最少连接算法就是将请求发送给当前最少连接数的服务器上**。

#### 4. 加权最少连接算法
在最少连接的基础上，根据服务器的性能为每台服务器分配**权重**，再根据权重计算出每台服务器能处理的连接数。

#### 5. 随机算法
把请求**随机发送到服务器**上。和轮询算法类似，该算法比较适合服务器**性能差不多**的场景。

#### 6. 源地址哈希算法
源地址哈希通过对**客户端 IP 计算哈希值**之后，再对**服务器数量取模**得到目标服务器的序号。可以保证同一 IP 的客户端的请求会转发到同一台服务器上，用来实现会话 **粘滞（Sticky Session）**。

### 1.5.2 转发实现
#### 1. HTTP重定向
HTTP 重定向负载均衡服务器使用某种负载均衡算法计算得到服务器的 IP 地址之后，将该地址写入 **HTTP 重定向报文**中，状态码为 **302**。客户端收到重定向报文之后，需要重新向服务器发起请求。

**缺点**：需要两次请求，因此访问延迟比较高；HTTP 负载均衡器处理能力有限，会限制集群的规模。该负载均衡转发的缺点比较明显，实际场景中**很少使用它**。

#### 2. DNS域名解析
在 **DNS 解析域名的同时使用负载均衡算法计算服务器 IP 地址**。

**优点**：DNS 能够根据地理位置进行域名解析，返回离用户最近的服务器 IP 地址。
**缺点**：由于 DNS 具有多级结构，每一级的域名记录都可能被缓存，当下线一台服务器需要修改 DNS 记录时，需要过很长一段时间才能生效。

大型网站基本使用了 DNS 做为 **第一级负载均衡** 手段，然后在内部使用其它方式做第二级负载均衡。也就是说，域名解析的结果为内部的负载均衡服务器 IP 地址。

#### 3. 反向代理服务器
**反向代理服务器**位于源服务器**前面**，用户的请求需要**先经过反向代理服务器**才能到达源服务器。反向代理可以用来**进行缓存、日志记录**等，同时也可以用来做为**负载均衡服务器**。

在这种负载均衡转发方式下，客户端不直接请求源服务器，因此源服务器不需要外部 IP 地址，而反向代理需要配置内部和外部两套 IP 地址。

**优点**：与其它功能集成在一起，部署简单。

**缺点**：**所有请求和响应** 都需要经过反向代理服务器，它可能会成为性能瓶颈。

#### 4. 网络层
在操作系统内核进程获取网络数据包，根据负载均衡算法计算源服务器的 IP 地址，并修改请求数据包的目的 IP 地址，最后进行转发。

源服务器返回的响应也需要经过负载均衡服务器，通常是让负载均衡服务器同时作为集群的网关服务器来实现。

**优点**：在内核进程中进行处理，性能比较高。

**缺点**：和反向代理一样，所有的请求和响应都经过负载均衡服务器，会成为性能瓶颈。

#### 5. 链路层
在链路层根据**负载均衡算法**计算源服务器的 **MAC** 地址，并修改请求数据包的目的 MAC 地址，并进行转发。

通过配置源服务器的虚拟 IP 地址和负载均衡服务器的 IP 地址一致，从而不需要修改 IP 地址就可以进行转发。也正因为 IP 地址一样，所以源服务器的响应不需要转发回负载均衡服务器，可以直接转发给客户端，避免了负载均衡服务器的成为瓶颈。

这是一种三角传输模式，被称为**直接路由**。对于提供下载和视频服务的网站来说，直接路由避免了大量的网络传输数据经过负载均衡服务器。

## 1.6 分布式锁
### 1.6.1 概述
在**单机**场景下，可以使用语言的**内置锁**来实现进程同步。但是在**分布式场景**下，需要同步的进程可能位于不同的节点上，那么就需要使用**分布式锁**。引入分布式锁来解决**分布式应用之间访问共享资源的并发问题**。**锁的本质**：**同一时间只允许一个用户操作**。

**阻塞锁** 通常使用 **互斥量** 来实现：
- 互斥量为 0 表示有其它进程在使用锁，此时处于锁定状态。
- 互斥量为 1 表示未锁定状态。

1 和 0 可以用一个**整型值**表示，也可以用**某个数据是否存在**表示。

### 1.6.2 数据库唯一索引
**获得锁** 时向表中 **插入一条记录**，**释放锁时删除这条记录**。**唯一索引** 可以保证该记录只被插入一次，那么就可以用这个记录是否存在来**判断是否存于锁定状态**。

存在以下几个问题：
- 锁**没有失效时间**，解锁失败的话其它进程无法再获得该锁。
- 只能是**非阻塞锁**，插入失败直接就报错了，无法重试。
- **不可重入**，已经获得锁的进程也必须重新获取锁。

MySQL 本身有自带的悲观锁 **for update** 关键字，也可以自己实现悲观/乐观锁来达到目的。

### 1.6.3 Redis的SETNX指令
#### 1. 概述
由于 Redis 是**单线程**，所以命令会以**串行**的方式执行，并且本身提供了像 SETNX(set if not exists) 这样的指令，本身具有互斥性。

使用 **SETNX**（set if not exist）指令插入一个键值对，如果 **Key 已经存在**，那么会返回 **False**，否则**插入成功并返回 True**。

SETNX 指令和数据库的**唯一索引类似**，保证了只存在一个 Key 的键值对，那么可以**用一个 Key 的键值对是否存在来判断是否存于锁定状态**。

EXPIRE 指令可以为一个键值对设置一个**过期时间**，从而避免了数据库唯一索引实现方式中**释放锁失败**的问题。

#### 2. SETNX源码分析
分布式锁类似于 "**占坑**"，而 **SETNX(SET if not exists)** 指令就是这样的一个操作，**只允许被一个客户端占有**。

实在**之前版本**的 Redis 中，由于 **SETNX** 和 **EXPIRE** 并不是 **原子指令**，所以在一起执行**会出现问题**。

也许你会想到使用 Redis 事务来解决，但在这里不行，因为 `EXPIRE` 命令**依赖**于 `SETNX` 的执行结果，而事务中没有 `if-else` 的分支逻辑，如果 `SETNX` 没有抢到锁，`EXPIRE` 就不应该执行。

为了解决这个疑难问题，Redis 开源社区涌现了许多分**布式锁的 library**，为了治理这个乱象，后来在 Redis 2.8 的版本中，**加入了 SET 指令的扩展参数，使得 SETNX 可以和 EXPIRE 指令一起执行了**, 只需要符合 **SET key value [EX seconds | PX milliseconds] [NX | XX] [KEEPTTL]** 这样的格式就好了。

另外，官方文档也在 SETNX 文档中提到了这样一种思路：**把 SETNX 对应 key 的 value 设置为 <current Unix time + lock timeout + 1>**，这样在其他客户端访问时就能够自己判断是否能够获取下一个 value 为上述格式的锁了。

#### 3. 代码实现
在 Redis 里使用 **SET key value [EX seconds] [PX milliseconds] NX** 创建一个 key，这样就算**加锁**。其中：
- **NX**：表示**只有 key 不存在**的时候才会设置**成功**，如果此时 Redis 中存在这个 key，那么设置失败，返回 nil。
- **EX seconds**：设置 **key 的过期时间**，精确到**秒级**。意思是 seconds 秒后**锁自动释放**，别人创建的时候如果发现已经有了就不能加锁了。
- **PX milliseconds**：同样是设置 key 的**过期时间**，精确到**毫秒级**。

**释放锁就是删除 key** ，但是一般可以用 lua 脚本删除，判断 value 一样才删除.

为啥要用 random_value **随机值** 呢？因为如果某个客户端获取到了锁，但是阻塞了很长时间才执行完，比如超过了 30s，此时可能已经自动释放锁了，此时可能别的客户端已经获取到了这个锁，要是这个时候直接删除 key 的话会有问题，所以得用随机值加上面的 lua 脚本来释放锁。

但是这样是肯定不行的。因为如果是普通的 Redis 单实例，那就是**单点故障**。或者是 Redis 普通主从，那 Redis 主从异步复制，如果主节点挂了（key 就没有了），key 还没同步到从节点，此时从节点切换为主节点，别人就可以 set key，从而拿到锁。

#### 4. 问题1：锁超时
假设有两平行的服务 A B，其中 A 服务在 **获取锁之后** 由于未知神秘力量突然 **挂了**，那么 B 服务就永远无法获取到锁了,所以需要额外设置一个 **超时时间**，来保证服务的**可用性**。

但是另一个问题随即而来：**如果在加锁和释放锁之间的业务逻辑执行得太长，以至于超出了锁的超时限制**，也会出现问题。因为这时候第一个线程持有锁过期了，而临界区的逻辑还没有执行完，与此同时第二个线程就提前拥有了这把锁，导致临界区的代码不能得到严格的串行执行。

为了避免这个问题，**Redis 分布式锁不要用于较长时间的任务**。如果真的偶尔出现了问题，造成的数据小错乱可能就需要人工的干预。

有一个稍微安全一点的方案是 **将锁的 value 值设置为一个随机数**，释放锁时**先匹配随机数是否一致**，然后**再删除 key**，这是为了 **确保当前线程占有的锁不会被其他线程释放**，除非这个锁是因为过期了而被服务器自动释放的。

**但是匹配 value 和删除 key 在 Redis 中并不是一个原子性的操作**，也没有类似保证原子性的指令，所以可能需要使用像 Lua 这样的脚本来处理了，因为 **Lua 脚本可以 保证多个指令的原子性执行**。

> **延伸的讨论：GC可能引发的安全问题**     

Martin Kleppmann 曾与 Redis 之父 Antirez 就 Redis 实现分布式锁的安全性问题进行过深入的讨论，其中有一个问题就涉及到 **GC**。在 GC 的时候会发生 **STW(Stop-The-World)**，这本身是为了保障垃圾回收器的正常执行，但可能会引发如下的问题：
<img src="assets/image-20200527154607741.png" alt="image-20200527154607741" style="zoom: 50%;" />

服务 A 获取了锁并设置了超时时间，但是**服务 A 出现了 STW 且时间较长，导致了分布式锁进行了超时释放**，在这个期间服务 B 获取到了锁，待服务 A STW 结束之后又恢复了锁，这就导致了 **服务 A 和服务 B 同时获取到了锁**，这个时候分布式锁就不安全了。

不仅仅局限于 Redis，Zookeeper 和 MySQL 有同样的问题。

#### 5. 问题2：单点/多点问题
如果 Redis 采用**单机部署模式**，那就意味着当 Redis 故障了，就会导致整个服务不可用。

而如果采用**主从模式**部署，想象一个这样的场景：服务 A 申请到**一把锁**之后，如果作为主机的 Redis 宕机了，那么服务 B 在申请锁的时候就会从**从机那里获取到这把锁**，为了解决这个问题，Redis 作者提出了一种 **RedLock 红锁** 的算法.

### 1.6.4 Redis的RedLock算法
#### 1. 概述
此种方式比原先的单节点的方法更安全。是 Redis 官方支持的**分布式锁算法**。

- **安全特性**：互斥访问，即永远只有一个 client 能拿到锁。
- **避免死锁**：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区。
- **容错性**：只要大部分 Redis **节点存活** 就可以正常提供服务。

#### 2. 算法流程
算法很易懂，起 **5 个 master 节点**，分布在不同的机房尽量保证可用性。为了**获得锁**，client 会进行如下操作：
1. 获取当前 **时间戳**，单位是毫秒。
2. 跟上面类似，轮流尝试在**每个 master 节点上创建锁**，过期时间较短，一般就几十毫秒。
3. 尝试在**大多数节点**上建立一个锁，比如 5 个节点就要求是 3 个节点 `n / 2 + 1` 。
4. 客户端计算**建立好锁的时间**，如果**建立锁的时间小于超时时间，就算建立成功**了。
5. 要是锁建立失败了，那么就**依次删除之前建立过的锁**。
6. 只要别人建立了一把分布式锁，就得**不断轮询去尝试获取锁**。

使用了**多个 Redis 实例**来实现分布式锁，这是为了保证在发生 Redis **单点故障** 时仍然可用。

#### 3. 失败重试
如果一个客户端**申请锁失败**了，那么它需要稍等一会再重试，避免多个客户端同时申请锁的情况，最好的情况是一个 client 需要**几乎同时向 5 个 master 发起锁申请**。另外就是如果 client 申请锁失败了它需要尽快在它曾经申请到锁的 master 上执行 **unlock 操作**，便于其他 client 获得这把锁，避免这些锁过期造成的时间浪费，当然如果这时候网络分区使得 client 无法联系上这些 master，那么这种浪费就是不得不付出的代价了。

#### 4. 释放锁
依次释放所有节点上的锁。

#### 5. 崩溃恢复和fsync
如果节点没有持久化机制，client 从 5 个 master 中的 3 个处获得了锁，然后其中一个重启了，注意 **整个环境中又出现了 3 个 master 可供另一个 client 申请同一把锁！** 违反了互斥性。如果开启了 AOF 持久化那么情况会稍微好转一些，因为 Redis 的过期机制是语义层面实现的，所以在 server 挂了的时候时间依旧在流逝，重启之后锁状态不会受到污染。但是考虑断电之后呢，AOF 部分命令没来得及刷回磁盘直接丢失了，除非配置刷盘策略为 fsnyc = **always**，但这会降低性能。

解决这个问题的方法是，当一个**节点重启**之后，规定**在 max TTL 期间它是不可用**的，这样它就不会干扰原本已经申请到的锁，等到它 crash 前的那部分锁都过期了，环境不存在历史锁了，那么再把这个节点加进来正常工作。

#### 6. 性能
Redlock 实在**不是**一个好的选择，对于需求性能的分布式锁应用**它太重了且成本高**；对于需求正确性的应用来说它不够安全。因为它对高危的时钟或者说其他上述列举的情况进行了不可靠的假设，如果**应用只需要高性能的分布式锁不要求多高的正确性，那么单节点 Redis 够了**；如果应用想要保住**正确性**，那么不建议 Redlock，建议使用一个**合适的一致性协调系统**，例如 Zookeeper，且保证存在 fencing token。

### 1.6.5 Zookeeper的有序节点
#### 1. Zookeeper抽象模型
Zookeeper 提供了一种**树形结构**的命名空间，/app1/p_1 节点的父节点为 /app1。

<img src="assets/image-20200528222019149.png" alt="image-20200528222019149" style="zoom: 50%;" />

#### 2. 节点类型
- **永久节点**：不会因为会话结束或者超时而消失。
- **临时节点**：如果会话结束或者超时就会消失。
- **有序节点**：会在节点名的后面加一个**数字后缀**，并且是**有序**的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，以此类推。

#### 3. 监听器
为一个**节点注册监听器**，在节点**状态发生改变**时，会给客户端**发送消息**。

#### 4. 分布式锁实现
- 创建一个**锁目录 /lock**。
- 当一个客户端需要获取锁时，在 /lock 下**创建临时的且有序的子节点**。
- 客户端获取 /lock 下的**子节点列表**，判断自己创建的**子节点是否为当前子节点列表中序号最小的子节点**，如果是则认为**获得锁**；否则**监听自己的前一个子节点**，获得子节点的变更通知后重复此步骤直至获得锁。
- 执行业务代码，完成后，**删除对应的子节点**。

#### 5. 会话超时
如果一个已经获得**锁的会话超时**了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其它会话就可以获得锁了。可以看到，Zookeeper 分布式锁**不会出现数据库的唯一索引实现的分布式锁释放锁失败问题**。

#### 6. 羊群效应
一个节点**未获得锁**，只需要**监听自己的前一个子节点**，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点**都会收到通知**（羊群效应），而我们只希望它的后一个子节点收到通知。

#### 7. 代码实现demo
zk 分布式锁，其实可以做的比较简单，就是**某个节点尝试创建临时 znode**，此时创建成功了就**获取了这个锁**；这个时候别的客户端来创建锁会失败，只能**注册个监听器**监听这个锁。释放锁就是删除这个 znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新加锁。

也可以采用另一种方式，创建**临时顺序节点**：

如果有一把锁，被**多个人**给竞争，此时**多个人会排队**，第一个拿到**锁的人会执行**，然后释放锁；后面的每个人都会去监听**排在自己前面**的那个人**创建的 node 上**，一旦某个人释放了锁，排在自己后面的人就会**被 ZooKeeper 给通知**，一旦被通知了之后，就 ok 了，自己就获取到了锁，就可以执行代码了。

### 1.6.6 Redis分布式锁和zk分布式锁的对比
- Redis 分布式锁，其实**需要自己不断去尝试获取锁**，比较消耗性能。
- zk 分布式锁，**获取不到锁，注册个监听器即可**，不需要不断主动尝试获取锁，性能开销较小。
- 如果是 Redis 获取锁的那个客户端 出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 zk 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就**自动释放锁**。Redis 分布式锁确实挺麻烦的，遍历上锁，计算时间等等，而 zk 的分布式锁语义清晰实现简单。

总体来说 zk 的分布式锁比 Redis 的分布式锁牢靠、而且模型简单易用。

## 1.7 分布式ID
随着业务发展，数据量将越来越大，需要对数据进行分表，而分表后如果每个表中的数据都会按自己的节奏进行常见的自增，很有可能出现 ID 冲突。这时就需要一个单独的**机制来负责生成唯一 ID**，生成出来的ID也可以叫做**分布式ID**，或**全局ID**。下面来分析各个生成分布式 ID 的机制。

生成分布式 ID 的思想大致两种：
- **自增思想**：数据库自增 ID、数据库多主模式、号段模式、Redis等。
- **雪花算法思想**。

### 1.7.1 UUID
不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。UUID 太长了、占用空间大，**作为主键性能太差** 了；更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。

### 1.7.2 数据库自增ID
基于数据库的自增 ID 需要 **单独使用一个数据库实例**，在这个实例中新建一个单独的表.虽然可行，但是基于性能与可靠性来考虑的话都不够，**业务系统每次需要一个 ID 时，都需要请求数据库获取，性能低，并且如果此数据库实例下线了，那么将影响所有的业务系统**。

### 1.7.3 数据库多主模式
为了解决上述的**数据库可靠性问题**，可以使用数据库多主模式分布式 ID 生成方案。

如果两个数据库组成一个**主从模式**集群，正常情况下可以解决数据库**可靠性问题**，但是如果主库挂掉后，数据没有及时同步到从库，这个时候会出现 **ID 重复**的现象。可以使用**双主模式**集群，也就是两个 MySQL 实例都能**单独的生产自增 ID**，这样能够提高效率，但是如果不经过其他改造的话，这两个 MySQL 实例很可能会生成同样的 ID。需要单独给每个MySQL 实例**配置不同的起始值和自增步长**。

对于这种生成分布式 ID 的方案，需要单独新增一个生成分布式 ID 应用，比如 DistributIdService，该应用提供**一个接口供业务应用获取 ID**，业务应用需要一个 ID 时，通过 **RPC 方式** 请求 DistributIdService，DistributIdService 随机去上面的两个 MySQL 实例中去获取 ID。

实行这种方案后，就算其中某一台 MySQL 实例下线了，也不会影响 DistributIdService，DistributIdService 仍然可以利用另外一台 MySQL 来生成 ID。

但是这种方案的**扩展性不太好**，如果两台 MySQL 实例不够用，需要新增 MySQL 实例来提高性能时，这时就会比较麻烦。为了解决上面的问题，以及能够进一步提高 DistributIdService 的性能，如果使用下面的生成分布式 ID 机制。

### 1.7.4 号段模式
可以使用号段的方式来获取自增 ID，号段可以理解成**批量获取**，比如 DistributIdService 从数据库**获取 ID 时**，如果能批量获取多个 ID 并**缓存在本地**的话，那样将大大提供业务应用获取 ID 的效率。

这种方案**不再强依赖数据库**，就算数据库不可用，那么 DistributIdService 也能继续支撑一段时间。但是如果 DistributIdService 重启，会丢失一段 ID，导致 **ID 空洞**。

为了提高 DistributIdService 的高可用，需要做一个集群，业务在请求 DistributIdService 集群获取 ID 时，会随机的选择某一个 DistributIdService 节点进行获取，对每一个 DistributIdService 节点来说，数据库连接的是**同一个数据库**，那么可能会产生多个 DistributIdService 节点同时请求数据库获取号段，那么这个时候需要利用**乐观锁**来进行控制，比如在数据库表中增加一个 version 字段，在获取号段时使用如下 SQL：
```
update id_generator set current_max_id=#{newMaxId}, version=version+1 where version = #{version}
```
因为 newMaxId 是 DistributIdService 中根据 **oldMaxId+步长** 算出来的，只要上面的 update 更新成功了就表示**号段获取成功**了。

为了提供数据库层的高可用，需要对**数据库使用多主模式进行部**署，对于每个数据库来说要保证生成的号段不重复，这就需要利用最开始的思路，再在刚刚的数据库表中增加起始值和步长.

在 **TinyId** 中还增加了一步来提高效率，在上面的实现中，ID 自增的逻辑是在 DistributIdService 中实现的，而实际上可以**把自增的逻辑转移到业务应用本地**，这样对于业务应用来说只需要获取号段，每次自增时**不再需要请求调用** DistributIdService 了。

### 1.7.5 Redis生成ID
使用 Redis 来生成分布式 ID 其实和利用 MySQL 自增 ID 类似，可以利用 Redis 中的 **incr 命令** 来实现 **原子性的自增与返回**.

使用 Redis 的效率是非常高的，但是要考虑**持久化**的问题。Redis 支持 RDB 和 AOF 两种持久化的方式。RDB 持久化相当于**打一个快照**进行持久化，如果打完快照后，连续自增了几次，还没来得及做下一次快照持久化，这个时候 Redis 挂掉了，重启 Redis 后会**出现 ID 重复**。AOF 持久化相当于对每条写命令进行持久化，如果 Redis 挂掉了，**不会出现 ID 重复的现象**，但是会由于 incr 命令过多而导致重启恢复数据时间过长。

### 1.7.6 雪花算法
#### 1. 概述
可以换个角度来对分布式 ID 进行思考，只要能让负责生成分布式ID的每台机器在每毫秒内生成不一样的ID就行了。

Snowflake 是 twitter 开源的分布式 ID 生成算法，所以它和上面的三种生成分布式 ID 机制不太一样，它**不依赖数据库**。

**核心思想是**：分布式 ID 固定是一个 **long 型数字**，一个 long 型占 8 个字节，也就是 **64 个 bit**，原始 snowflake 算法中对于 bit 的分配如下图：
<img src="assets/image-20200726135356181.png" alt="image-20200726135356181" style="zoom:57%;" />

- **第一个 bit 位**：是标识部分，在 Java 中由于 long 的最高位是符号位，正数是 0，负数是 1，一般生成的 ID 为正数，所以不使用且固定为 0。
- **时间戳部分**：占 41 bit，这个是**毫秒级的时间**，一般实现上不会存储当前的时间戳，而是时间戳的差值（当前时间 - 固定的开始时间），这样可以使**产生的 ID 从更小值**开始；41 位的时间戳可以使用 69 年，(1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69年。
- **工作机器 id**：占 10bit，这里比较灵活，比如，可以使用前 5 位作为数据中心机房标识，后 5 位作为单机房机器标识，可以部署 1024 个节点。
- **序列号部分**：占 12bit，支持**同一毫秒内**同一个节点可以生成 4096 个 ID。

许多大厂其实并没有直接使用 snowflake，而是进行了**改造**，因为 snowflake 算法中**最难实践的就是工作机器 id**，原始的雪花算法需要**人工去为每台机器去指定一个机器 id**，并配置在某个地方从而让算法从此处获取机器 id。但是在大厂里，机器是很多的，人力成本太大且容易出错，所以许多都进行了改造。

#### 2. 百度(uid-generator)
**uid-generator** 使用的就是 snowflake，只是在生产机器 id，也叫做 workId 时有所不同。

#### 3. 美团(Leaf)
美团的 Leaf 也是一个分布式 ID 生成框架。它非常全面，即**支持号段模式**，也支持 snowflake 模式。

总得来说，上面两种都是自动生成 workId，以让**系统更加稳定以及减少人工成功**。

# 2. 一致性算法
Raft 算法框架---Etcd,它的功能类似于 Zookeeper，但采用了更为主流的 Rest 接口。
## 2.1 2PC（两阶段提交）
两阶段提交是一种保证**分布式系统数据一致性**的协议，现在很多**数据库都是采用的两阶段提交协议**来完成 **分布式事务** 的处理。PC 是 phase-commit 的缩写，即**阶段提交**。

在两阶段提交中，主要涉及到两个角色，分别是 **协调者和参与者**。

**第一阶段**：当要执行一个分布式事务的时候，**事务发起者首先向协调者发起事务请求**，然后协调者会给所有参与者发送 **prepare 请求**（其中包括事务内容）告诉参与者需要执行事务了，如果能执行事务内容那么就**先执行但不提交**，执行后请回复协调者。然后参与者收到 prepare 消息后会开 **始执行事务（但不提交）** ，并将 **Undo 和 Redo 信息记入事务日志** 中，之后参与者就 **向协调者反馈是否准备** 好了。

**第二阶段**：第二阶段主要是**协调者根据参与者反馈的情况来决定接下来是否可以进行事务的提交操作**，即提交事务或者回滚事务。比如这个时候 所有的参与者 **都返回了准备好了的消息**，这个时候就进行事务的提交，协调者此时会给所有的参与者发送 **Commit 请求**，当参与者收到 Commit 请求的时候会执行前面执行的事务的 **提交操作** ，提交完毕之后将给协调者发送**提交成功的响应**。而如果在第一阶段有参与者**执行事务失败**，那么此时协调者将会给所有参与者发送 **回滚事务的 rollback 请求**，所有参与者收到之后将会 **回滚它在第一阶段所做的事务处理** ，然后再将处理情况返回给协调者，最终协调者收到响应后便给事务发起者返回处理失败的结果。

事实上 2PC 只解决了各个事务的**原子性问题**，随之也带来了很多的问题：
- **单点故障问题**：如果协调者挂了那么整个系统都处于不可用的状态了。
- **阻塞问题**：即当协调者发送 prepare 请求，参与者收到之后如果能处理那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放，如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。
- **数据不一致问题**：比如当第二阶段，协调者只发送了一部分的 commit 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题。

## 2.2 3PC（三阶段提交）
因为 2PC 存在的一系列问题，比如单点，容错机制缺陷等等，从而产生了 **3PC（三阶段提交）** 。这三个阶段就是：
1. **CanCommit 阶段**：**协调者** 向所有参与者发送 **CanCommit 请求**，参与者收到请求后会根据自身情况查看 **是否能执行事务**，如果可以则返回 YES 响应并进入 **预备状态**，否则返回 NO 。
2. **PreCommit 阶段**：协调者根据参与者返回的响应来**决定是否可以进行下面的 PreCommit 操作**。如果上面参与者返回的都是 YES，那么**协调者将向所有参与者发送 PreCommit 预提交请求**，参与者收到预提交请求后，会进行**事务的执行**操作，并将 **Undo 和 Redo 信息写入事务日志** 中 ，最后如果参与者顺利执行了事务则给协调者 **返回成功的响应**。如果在第一阶段协调者收到了 任何一个 NO 的信息，或者 在一定时间内 并没有收到全部的参与者的响应，那么就会**中断事务**，它会向所有参与者发送**中断请求**（abort），参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，它也会中断事务。
3. **DoCommit 阶段**：这个阶段其实和 2PC 的第二阶段差不多，如果协调者收到了所有参与者在 PreCommit 阶段的 **YES 响应**，那么协调者将会给所有参与者发送 **DoCommit 请求**，参与者收到 DoCommit 请求后则会进行事务的提交工作，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。若协调者在 PreCommit 阶段 收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应 ，那么就会进行中断请求的发送，参与者收到中断请求后则会 通过上面记录的**回滚日志 来进行事务的回滚操作**，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。

3PC 在很多地方进行了**超时中断的处理**，比如协调者在指定时间内为收到全部的确认消息则进行事务中断的处理，这样能 **减少同步阻塞** 的时间 。还有需要注意的是，3PC 在 DoCommit 阶段参与者如未收到协调者发送的提交事务的请求，**它会在一定时间内进行事务的提交**。为什么这么做呢？是因为这个时候肯定保证了在第一阶段所有的协调者全部返回了可以执行事务的响应，这个时候**有理由相信其他系统都能进行事务的执行和提交**，所以不管协调者有没有发消息给参与者，进入第三阶段参与者都会进行事务的提交操作。

总之，3PC 通过一系列的 **超时机制很好的缓解了阻塞问题**，但是最重要的 **一致性并没有得到根本的解决**，比如在 PreCommit 阶段，当一个参与者收到了请求之后其他参与者和协调者挂了或者出现了网络分区，这个时候收到消息的参与者都会进行事务提交，这就会出现数据不一致性问题。所以，要解决一致性问题还需要靠 **Paxos 算法** 。

## 2.3 Paxos算法
### 2.3.1 概述
Paxos 算法是一种 **基于消息传递的分布式一致性算法**，Paxos 的最大特点就是难，**不仅难以理解，更难以实现**。

Paxos 算法是基于**消息传递且具有高度容错特性的一致性算法**，是目前公认的解决分布式一致性问题最有效的算法之一，**其解决的问题就是在分布式系统中如何就某个值（决议）达成一致** 。对多个节点产生的值，该算法能**保证只选出唯一一个值**。

Paxos 算法运行在**允许宕机故障的异步系统**中，**不要求可靠的消息传递**，可容忍消息丢失、延迟、乱序以及重复。它利用大多数 (Majority) 机制保证了 2F+1 的容错能力，**即 2F+1个 节点的系统最多允许 F 个节点同时出现故障**。

一个或多个**提议进程 (Proposer)** 可以发起**提案 (Proposal)**，Paxos 算法使所有提案中的某一个提案，在**所有进程中达成一致**。系统中的多数派同时认可该提案，即达成了一致。最多只针对一个确定的提案达成一致。

主要有三类节点：
- **提议者**（Proposer）：提议一个值。
- **接受者**（Acceptor）：对每个提议进行投票。
- **最终决策学习者**（Learner）：被告知投票的结果，不参与投票过程。

在多副本状态机中，每个副本同时具有 Proposer、Acceptor、Learner **三种角色**。

### 2.3.2 执行过程
Paxos 算法通过一个决议分为**两个阶段**（Learn 阶段之前决议已经**形成**）。规定一个提议包含两个字段：[n, v]，其中 n 为序号（具有唯一性），v 为提议值。
1. **第一阶段**：**Prepare 阶段**。Proposer 向 Acceptors 发出 Prepare 请求，Acceptors 针对收到的 **Prepare 请求**进行 **Promise 承诺**。
2. **第二阶段**：**Accept 阶段**。Proposer 收到多数 Acceptors 承诺的 Promise 后，向 Acceptors 发出 **Propose 请求**，Acceptors 针对收到的 Propose 请求进行 **Accept 处理**。
3. **第三阶段**：**Learn 阶段**。Proposer 在收到多数 Acceptors 的 Accept 之后，标志着**本次 Accept 成功**，决议形成，将形成的决议发送**给所有 Learners**。

### 2.3.3 约束条件
#### 1. 正确性
指**只有一个提议值会生效**。因为 Paxos 协议要求每个生效的提议被多数 Acceptor 接收，并且 Acceptor 不会接受两个不同的提议，因此可以保证正确性。
#### 2. 可终止性
指最后**总会有一个提议生效**。Paxos 协议能够让 Proposer 发送的提议朝着能被大多数 Acceptor 接受的那个提议靠拢，因此能够保证可终止性。

### 2.3.4 Paxos算法的死循环问题
比如说，此时提案者 P1 提出一个方案 **M1**，完成了 **Prepare 阶段** 的工作，这个时候 acceptor 则批准了 M1，但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 Prepare 阶段的工作。然后 P1 的方案已经不能在第二阶段被批准了（因为 acceptor 已经批准了比 M1 更大的 M2），所以 P1 自增方案变为 M3 重新进入 Prepare 阶段，然后 acceptor ，又批准了新的 M3 方案，它又不能批准 M2 了，这个时候 M2 又自增进入 Prepare 阶段。

就这样无休无止的永远提案下去，这就是 Paxos 算法的死循环问题。

## 2.4 Raft算法
Raft 是一个通俗易懂，更**容易落地**的分布式协议。

### 2.4.1 节点的状态
每个节点有**三个状态**，他们会在这三个状态之间进行变换。**客户端只能从主节点写数据，从节点里读数据**。
<img src="assets/image-20200726165404191.png" alt="image-20200726165404191" style="zoom:60%;" />

### 2.4.2 流程分析
#### 1. 选主流程
初始是 **Follwer 状态节点**，等 100-300MS 没有收到 LEADER 节点的心跳就**变候选人**。候选人给大家发选票，候选人获得大多数节点的选票就变成了 LEADER 节点。
#### 2. 日志复制流程
每次改变数据**先记录日志**，**日志未提交不能改节点的数值**。然后 LEADER 会复制数据给其他 follower 节点，并等大多数节点写日志成功再提交数据。
#### 3. 选举超时
每个节点随机等 150 到 300MS，如果时间到了就开始发选票，因为有的节点等的时间短，所以它会先发选票，从而当选成主节点。但是如果两个候选人获得的票一样多，它们之间就要打加时赛，这个时候又会重新随机等 150 到 300MS，然后发选票，直到获得最多票当选成主节点。
#### 4. 心跳超时
每个节点会记录主节点是谁，并且和主节点之间维持一个心跳超时时间，如果没有收到主节点回复，从节点就要重新选举候选人节点。
#### 5. 集群中断
当集群之间的部分节点失去通讯时，主节点的日志不能复制给多个从节点就不能进行提交。
#### 6. 集群恢复
当集群恢复之后，原来的主节点发现自己不是选票最多的节点，就会变成**从节点**，并回滚自己的日志，最后主节点会同步日志给从节点，保持主从数据的一致性。

# 3. 分布式事务
**分布式事务** 就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的**不同节点之上**。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，**分布式事务就是为了保证不同数据库的数据一致性**。

## 3.1 概述
### 3.1.1 引入
普通数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性 ( Isolation)和持久性(Durabilily)，简称 **ACID**。当**单个数据库**的性能产生瓶颈的时候，可能会对**数据库进行分区**，这里所说的分区指的是**物理分区**，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的 ACID 已经**不能适应**这种情况了，而在这种 ACID 的集群环境下，再想保证集群的 ACID 几乎是很难达到，分布式下追求集群的 ACID 会导致系统性能变得很差，这时就需要引入一个**新的理论**原则来适应这种集群的情况，就是 **CAP 定理**。分布式系统往往追求的是**可用性**，它的重要程序**比一致性要高**，那么如何实现高可用性呢？ 就是 **BASE 理论**，它是用来对 CAP 定理进行进一步扩充的。

BASE 理论是对 CAP 中的**一致性和可用性进行一个权衡**的结果，理论的核心思想就是：**无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性**（Eventual consistency）。

因为**分布式系统的核心就是处理各种异常情况，这也是分布式系统复杂的地方**，因为分布式的网络环境很复杂，这种“断电”故障要比单机多很多，这些异常可能有 **机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP、存储数据丢失、其他异常** 等。

分布式所需要解决的是在分布式系统中，整个调用链中，所有服务的数据处理要么都成功要么都失败，即所有服务的**原子性问题** 。

### 3.1.2 分布式事务的应用场景
#### 1. 支付
#### 2. 在线下单

在分布式系统中，要实现分布式事务，**解决方案** 有下面几种。
## 3.2 两阶段提交2PC
**两阶段提交**（Two-phase Commit，2PC），通过**引入协调者**（Coordinator）来协调参与者的行为，并最终决定这些参与者**是否要真正执行事务**。和上一节中提到的**数据库 XA 事务**一样，两阶段提交就是使用 **XA 协议** 的原理。**2PC 基于 XA 协议的两阶段提交**。

## 3.3 基于消息中间件的2PC
**消息事务+最终一致性**。

所谓的消息事务就是**基于消息中间件**的**两阶段提交**，本质上是对消息中间件的一种特殊利用，它是将**本地事务和发消息放在了一个分布式事务**里，保证要么本地操作成功成功并且对外发消息成功，要么两者都失败，开源的 RocketMQ 就支持这一特性，但是市面上一些主流的 MQ 都是不支持事务消息的，比如 **RabbitMQ 和 Kafka 都不支持**。

以阿里的 RocketMQ 中间件为例，其思路大致为：
- 第一阶段 **Prepared** 消息，会拿到**消息的地址**。
- 第二阶段**执行本地事务**。
- 第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。

也就是说在业务方法内要想消息队列提交两次请求，一次发送消息和一次确认消息。如果确认消息发送失败了 RocketMQ 会定期扫描消息集群中的事务消息，这时候发现了 Prepared 消息，它会向消息发送者确认，所以生产方需要实现一个 check 接口，RocketMQ 会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。

过程如下：
- A 系统向消息中间件发送一条**预备消息**。
- 消息中间件**保存预备消息并返回成功**。
- A **执行本地事务**。
- A 发送**提交消息**给消息中间件。

通过以上 4 步完成了一个**消息事务**。对于以上的 4 个步骤，每个步骤都可能产生错误，下面一一分析：
- 步骤一出错，则整个事务失败，不会执行 A 的本地操作。
- 步骤二出错，则整个事务失败，不会执行 A 的本地操作。
- 步骤三出错，这时候需要**回滚预备消息**，怎么回滚？答案是 A 系统实现一个消息中间件的回调接口，消息中间件会去不断执行回调接口，检查 A 事务执行是否执行成功，如果失败则回滚预备消息。
- 步骤四出错，这时候 A 的**本地事务是成功**的，那么消息中间件要回滚 A 吗？答案是不需要，其实通过回调接口，消息中间件能够检查到 A 执行成功了，这时候其实不需要 A 发提交消息了，消息中间件可以自己对消息进行提交，从而完成整个消息事务。

**基于消息中间件的两阶段提交往往用在高并发场景**下，将一个分布式事务拆成一个**消息事务**（A 系统的本地操作+发消息）+ B 系统的本地操作，其中 B 系统的操作由消息驱动，只要消息事务成功，那么 A 操作一定成功，消息也一定发出来了，这时候 B 会收到消息去执行本地操作，如果本地操作失败，消息会重投，直到 B 操作成功，这样就变相地实现了 A 与 B 的分布式事务。原理如下：
<img src="assets/image-20200529122629725.png" alt="image-20200529122629725" style="zoom:87%;" />

虽然上面的方案能够完成 A 和 B 的操作，但是 A 和 B 并不是严格一致的，而是**最终一致**的，在**这里牺牲了一致性，换了性能的大幅度提升**。当然，这种做法也是有风险的，如果 B 一直执行不成功，那么一致性会被破坏，具体要不要，还是得看业务能够承担多少风险。

## 3.4 补偿事务TCC
所谓的 TCC 编程模式，也是**两阶段提交的一个变种**。TCC 提供了一个编程框架，将整个业务逻辑分为三块：**Try、Confirm 和 Cancel 三个操作**。其核心思想是：**针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作**。它分为三个阶段：
- **Try 阶段** 主要是对业务系统做检测及资源预留。
- **Confirm 阶段** 主要是对业务系统做确认提交，Try 阶段执行成功并开始执行 Confirm 阶段时，默认 Confirm 阶段是不会出错的。即：只要 Try 成功，Confirm 一定成功。
- **Cancel 阶段** 主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。

TCC 在 2, 3 步中都有可能失败。TCC 属于应用层的一种**补偿方式**，所以**需要程序员在实现的时候多写很多补偿的代码**，在一些场景中，一些业务流程可能用 TCC 不太好定义及处理。

## 3.5 本地消息表(异步确保)
本地消息表与业务数据表处于**同一个数据库**中，这样就能利用**本地事务**来保证在对这两个表的操作满足事务特性，并且使用了**消息队列来保证最终一致性**。

本地消息表这种实现方式应该是**业界使用最多**的，其核心思想是 **将分布式事务拆分成本地事务进行处理**，这种思路是来源于 **ebay**。
1. 在分布式事务操作的 **一方完成写业务数据的操作之后向本地消息表发送一个消息**，本地事务能保证这个消息一定会被 **写入本地消息表** 中。
2. 之后将**本地消息表中的消息转发到消息队列**中，如果转发成功则将**消息从本地消息表中删除**，否则继续重新转发。
3. 在分布式事务操作的**另一方从消息队列中读取一个消息，并执行消息中的操作**。

**基本思路就是**：消息**生产方**，需要**额外建一个消息表**，**并记录消息发送状态**。消息表和业务数据要在一个事务里提交，也就是说他们要在**一个数据库里面**。然后消息会经过 MQ 发送到消息的消费方，如果消息发送失败，会进行重试发送。

**消息消费方** 需要处理这个消息，并完成自己的业务逻辑。此时如果 **本地事务处理成功**，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是上面的业务失败，可以给生产方发送一个**业务补偿消息**，通知生产方进行回滚等操作。

生产方和消费方 **定时扫描本地消息表**，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。

这种方案遵循 **BASE 理论**，采用的是 **最终一致性**，是这几种方案里面比较适合实际业务场景的，即不会出现像 2PC 那样复杂的实现(当调用链很长的时候，2PC 的可用性是非常低的)，也不会像 TCC 那样可能出现确认或者回滚不了的情况。

**优点：** 一种非常经典的实现，避免了分布式事务，实现了最终一致性。在 .NET 中 有现成的解决方案。

**缺点：** **消息表** 会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。

## 3.6 总结
分布式事务，本质上是对多个数据库的事务进行统一控制，按照控制力度可以分为：不控制、部分控制和完全控制。不控制就是不引入分布式事务，部分控制就是各种变种的两阶段提交，包括上面提到的消息事务+最终一致性、TCC 模式，而完全控制就是完全实现两阶段提交。部分控制的好处是并发量和性能很好，缺点是数据一致性减弱了，完全控制则是牺牲了性能，保障了一致性，具体用哪种方式，最终还是取决于业务场景。

**选用经验**：对于特别严格的场景（比如涉及**金钱操作**），可以 **TCC 来保证强一致性**；然后其他的一些场景可以基于阿里的 RocketMQ 来实现分布式事务。

# 4. 系统认证与授权
## 4.1 认证与授权
### 4.1.1 基本概念
**认证(Authentication)** 是**验证用户身份的凭据**（例如用户名/用户 ID 和密码），通过这个凭据，系统得以知道存在这个用户。所以 **Authentication** 被称为身份/用户验证。

**授权(Authorization)** 发生在 **Authentication（认证）** 之后。授权就是控制访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问比如 admin，有些对系统资源操作比如删除、添加、更新只能特定人才具有。

### 4.1.2 Cookie与Session
Cookie 和 Session 都是用来**跟踪浏览器用户身份**的会话方式，但是两者的应用场景不太一样。
#### 1. Cookie
##### (1) 概述
Cookie 存放在客户端，一般用来保存用户信息。
##### (2) Cookie使用场景
1. 在 Cookie 中保存已经**登录过的用户信息**，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了。除此之外，Cookie 还能保存用户首选项，主题和其他设置信息。
2. 因为 HTTP 协议是无状态的，所以可以使用 Cookie **保存 session 或者 token** ，向后端发送请求的时候**带上 Cookie**，这样后端就能取到 session 或者 token，这样就能记录用户当前的状态了。
3. Cookie 还可以用来**记录和分析用户行为**。比如网购时服务器想要获取用户在某个页面的停留状态或者商品浏览记录，一种常用的实现方式就是将这些信息存放到 Cookie 中。
##### (3) 服务端使用Cookie

#### 2. Session
**Session 的主要作用就是通过服务端记录用户的状态**。典型的场景是**购物车**，当要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建**特定的 Session 之后**就可以标识这个用户并且跟踪这个用户了。

>**如何使用 Session 进行身份验证？**

一般通过 **SessionID** 来实现用户的验证，SessionID 一般会选择存放在 **Redis** 中。举个例子：用户成功登陆系统，然后返回给客户端具有 **SessionID 的 Cookie**，，当用户向后端发起请求的时候会把 SessionID 带上，这样后端就知道用户的身份状态了。关于这种认证方式更详细的过程如下：
1. 用户向服务器发送用户名和密码用于登陆系统。
2. 服务器验证通过后，服务器为用户**创建一个 Session**，并将 Session 信息**存储**起来。
3. 服务器向用户返回一个 **SessionID**，**写入用户的 Cookie**。
4. 当用户保持登录状态时，Cookie 将与**每个后续请求**一起被发送出去。
5. 服务器可以将存储在 Cookie 上的 Session ID 与存储在内存中或者数据库中的 Session 信息进行**比较**，以验证用户的身份，返回给用户客户端响应信息的时候会附带用户当前的状态。

使用 Session 的时候需要注意：
1. 依赖 Session 的关键业务一定要**确保客户端开启了 Cookie**。
2. 注意 Session 的**过期时间**。

#### 3. Cookie与Session区别
**Cookie** 数据保存在**客户端**(浏览器端)，**Session** 数据保存在**服务器端**。相对来说 Session 安全性更高。如果使用 Cookie 的一些敏感信息不要写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。

#### 4. 常见问题
> **如果没有Cookie的话Session还能用吗？**

一般是通过 **Cookie 来保存 SessionID** ，假如使用了 Cookie 保存 SessionID 的方案的话， 如果客户端**禁用了 Cookie**，那么 Seesion 就**无法**正常工作。

但是并不是没有 Cookie 之后就不能用 Session 了，比如**可以将 SessionID 放在请求的 URL 里面。** 。这种方案的安全性和用户体验感较低。也可以对  SessionID 进行一次加密之后再传入后端。

> **为什么Cookie无法防止CSRF攻击，而token可以？**

**CSRF（Cross Site Request Forgery）** 一般被翻译为 **跨站请求伪造** 。上面也提到进行 Session 认证的时候，一般使用 Cookie 来存储 SessionId，当登陆之后后端生成一个 SessionId 放在 **Cookie** 中返回给客户端，服务端通过 Redis 或者其他存储工具记录保存着这个 SessionId，客户端登录以后每次请求都会**带上这个 SessionId**，服务端通过这个 SessionId 来标示你这个人。如果别人通过 cookie 拿到了 sessionId 后就可以冒充用户的身份访问系统了。

Session 认证中 **Cookie 中的 SessionId 是由浏览器** 发送到服务端的，借助这个特性，攻击者就可以通过 **让用户误点攻击链接**，达到攻击效果。

但是使用 **token** 的话就不会存在这个问题，在登录成功获得 token 之后，一般**会选择存放在 local storage** 中。然后在前端通过**某些方式**会给每个发到后端的请求**加上这个 token**，这样就不会出现 CSRF 漏洞的问题。因为即使用户点击了非法链接发送了请求到服务端，这个**非法请求是不会携带 token** 的，所以这个请求将是**非法**的。

**需要注意的是不论是 Cookie 还是 token 都无法避免跨站脚本攻击（Cross Site Scripting）XSS**。跨站脚本攻击（Cross Site Scripting）缩写为 CSS 但这会与层叠样式表（Cascading Style Sheets，CSS）的缩写混淆。因此，有人将跨站脚本攻击缩写为XSS。XSS 中攻击者会用各种方式将**恶意代码**注入到其他用户的页面中，就可以通过**脚本**盗用信息，使用脚本的话 cookie 和 token 都可能被盗用。

## 4.2 Token与JWT
### 4.2.1 概述
可以使用 Session 来鉴别用户的身份，Session  信息需要保存一份在服务器端。这种方式会带来一些麻烦，比如需要保证保存  Session  信息服务器的可用性、**不适合移动端**（依赖 Cookie）等等。

使用 **Token** 可以不需要服务端存放 Session 信息就能实现身份验证。**JWT （JSON Web Token）** 就是这种方式的实现，通过这种方式服务器端就不需要保存 Session 数据了，**只用在客户端保存服务端返回给客户的 Token** 就可以了，扩展性得到提升。

**JWT 本质上就一段签名的 JSON 格式的数据。由于它是带有签名的，因此接收者便可以验证它的真实性**。

JWT 由 3 部分构成：
1. **Header**（头部）：描述 JWT 的元数据。定义了生成**签名**的算法以及 Token 的类型。
2. **Payload**（负载）：用来存放实际**需要传递**的数据。
3. **Signature**（签名）：服务器通过 Payload、Header 和一个密钥(secret) 使用 Header 里面指定的签名算法（默认是 HMAC SHA256）生成。

在基于 Token 进行身份验证的的应用程序中，服务器通过 Payload、Header 和一个密钥(secret)创建**令牌（Token）**并将 Token 发送给客户端，客户端将 Token **保存在 Cookie 或者 localStorage 里面**，以后客户端发出的所有请求都会**携带**这个令牌 token。客户端可以把 token 放在 **Cookie** 里面**自动发送**，但是这样**不能跨域**，所以更好的做法是**放在 HTTP  Header** 的  **Authorization** 字段中：**Authorization: Bearer Token**。

详细流程如下：
1. 用户向服务器发送用户名和密码用于登陆系统。
2. 身份验证服务响应并返回了**签名的 JWT**，里面包含了用户信息。
3. 用户以后每次向后端发请求都在 **Header 中带上 JWT**。
4. 服务端检查 JWT 并从中获取用户相关信息并进行**验证**。

### 4.2.2 Token认证的优势
相比于 Session 认证的方式来说，使用 token 进行身份认证主要有下面三个优势：

#### 1. 无状态
token 自身包含了**身份验证**所需要的所有信息，使得服务器**不需要存储 Session 信息**，这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。但是也正是由于 token 的无状态，也导致了它最大的缺点：当后端在 token 有效期内废弃一个 token 或者更改它的权限的话，**不会立即生效**，一般需要等到有效期过后才可以。另外当用户 Logout 的话，token 也还有效。除非在后端增加额外的处理逻辑。

#### 2. 可以避免CSRF攻击
**CSRF**一般被翻译为 **跨站请求伪造**，那为什么使用 token 不会存在这种问题呢？

token 一般会选择存放在客户端的 **LocalStorage** 中，这样就不会出现 CSRF 漏洞问题。因为即使有个用户点击了非法链接发送了请求到服务端，这个非法请求是**不会**携带 token 的，所以这个请求将是非法的。

但是这样会存在 XSS 攻击中被盗的风险，为了避免 XSS 攻击，可以选择将 token 存储在标记为 **httpOnly** 的 cookie 中，但是这样又导致了必须自己提供 CSRF 保护。

具体采用上面哪种方式存储 token 呢，**大部分**情况下存放在 **LocaStorage** 下都是比较好的选择，某些情况下可能需要存放在标记为 httpOnly 的cookie 中会更好。

#### 3. 适合移动端应用
使用 Session 进行身份认证的话，需要保存一份信息在**服务器端**，而且这种方式会**依赖到 Cookie**（需要 Cookie 保存 SessionId），所以**不适合移动端**。但是，使用 token 进行身份认证就不会存在这种问题，因为只要 token 可以被客户端存储就能够使用，而且 token 还可以跨语言使用。

#### 4. 单点登录友好
使用 Session 进行身份认证的话，实现单点登录，需要把用户的 Session 信息保存在**一台电脑**上，并且还会遇到常见的 **Cookie 跨域**的问题。但是使用 token 进行认证的话， token 被**保存在客户端**，不会存在这些问题。

### 4.2.3 Token认证常见问题以及解决办法
#### 1. 注销登录等场景下token还有效
与之**类似**的具体相关场景有：
1. 退出登录。
2. 修改密码。
3. 服务端修改了某个用户具有的权限或者角色。
4. 用户的帐户被删除/暂停。
5. 用户由管理员注销；

这个问题**不存在于 Session 认证方式**中，因为在 Session 认证中，遇到这种情况的话服务端**删除对应的 Session 记录即可**。但是，使用 token 认证的方式就不好解决了。token 一旦派发出去，如果后端不增加其他逻辑的话，它在**失效之前都是有效**的。那么如何解决这个问题呢？大致有下面几种方案：
- **将 token 存入内存数据库**：将 token 存入 DB 中，如 Redis。如果需要让某个 **token 失效**就直接从 redis 中**删除这个 token 即可**。但是这样会导致每次使用 token 发送请求都要先从 DB 中查询 token 是否存在的步骤，而且违背了 JWT 的**无状态原则**。
- **黑名单机制**：和上面的方式类似，使用内存数据库比如 Redis 维护一个**黑名单**，如果想让某个 token 失效的话就直接将这个 token 加入到 **黑名单** 即可。然后，每次使用 token 进行请求的话都会先判断这个 token **是否存在于黑名单**中。
- **修改密钥 (Secret)** : 为每个用户都创建一个**专属密钥**，如果想让某个 token 失效，直接修改**对应用户的密钥**即可。但是这样相比于前两种引入内存数据库带来了危害更大，比如：1. 如果服务是**分布式**的，则每次发出新的 token 时都必须在**多台机器同步密钥**。为此需要将必须将机密存储在数据库或其他外部服务中，这样和 Session 认证就没太大区别了。2. 如果用户同时在两个浏览器打开系统，或者在手机端也打开了系统，如果它从一个地方将账号退出，那么其他地方**都要**重新进行登录，这是不可取的。
- **保持令牌的有效期限短并经常轮换** ：很简单的一种方式。但是，会导致用户登录状态不会被持久记录，而且需要用户**经常登录**。京东招聘的官网就是这样，很难受。

对于**修改密码后** token 还有效问题的解决还是比较容易的，一种比较好的方式：**使用用户的密码的哈希值对 token 进行签名。因此，如果密码更改，则任何先前的令牌将自动无法验证**。

#### 2. token续签问题
token 有效期一般都建议设置的不太长，那么 token 过期后如何认证，如何**实现动态刷新 token**，避免用户经常需要重新登录？

先来看看在 Session 认证中一般的做法：假如 session 的有效期 30 分钟，如果 30 分钟内用户有访问，就把 session 有效期被延长 30 分钟。那 token 续签怎么做？
- **类似于 Session 认证中的做法**：这种方案满足于大部分场景。假设服务端给的 token 有效期设置为 30 分钟，服务端每次进行**校验时**，如果发现 token 的有效期马上快过期了，服务端就**重新生成 token 给客户端**。客户端每次请求都检查新旧 token，如果**不一致**，则**更新**本地的 token。这种做法的问题是仅仅在**快过期**的时候请求才会更新 token，对客户端不是很友好。
- **每次请求都返回新 token**：这种方案的的思路很简单，但是服务端**开销会比较大**。
- **token 有效期设置到半夜** ：这是一种折衷的方案，保证了大部分用户白天可以正常登录，适用于对安全性要求不高的系统。
- **用户登录返回两个 token** ：第一个是 **acessToken** ，它的**过期时间 token 本身的过期时间比如半个小时**，另外一个是 **refreshToken** 它的过期时间**更长一点**比如为 1 天。客户端登录后，将 accessToken 和 refreshToken 保存在**本地**，每次访问将 accessToken 传给服务端。服务端校验 accessToken 的**有效性**，如果**过期**的话，就将 **refreshToken** 传给服务端。如果有效，服务端就生成**新的 accessToken 给客户端**。否则，客户端就重新登录即可。该方案的不足是：1. 需要客户端来**配合**；2. 用户注销的时候需要同时**保证两个 token 都无效**；3. 重新请求获取 token  的过程中会有**短暂 token 不可用**的情况（可以通过在客户端设置定时器，当 accessToken 快过期的时候，提前去通过 refreshToken 获取新的 accessToken）。

## 4.3 OAuth2.0
OAuth 是一个行业的**标准授权协议**，主要用来**授权第三方应用获取有限的权限**。它的最终目的是**为第三方应用颁发一个有时效性的令牌 token**，使得第三方应用能够**通过该令牌获取相关的资源**。

## 4.4 单点登录SSO
### 4.4.1 概述
SSO(Single Sign On) 即**单点登录**，指的是用户**登陆多个子系统的其中一个就有权访问与其相关的其他系统**。比如登陆京东金融之后，京东超市、京东家电等子系统都是登录状态。

#### 1. SSO与OAuth2.0的区别
OAuth 是一个行业的标准**授权协议**，主要用来**授权第三方应用获取有限的权限**。而 SSO 解决的是一个公司的**多个相关的子系统的之间的登陆问题**比如京东旗下相关子系统京东金融、京东超市、京东家电等等。

#### 2. 单点登录系统的好处
1. **用户角度**：用户能够做到一次登录多次使用，非常友好。
2. **系统管理员角度**：系统只需维护一个统一的账号中心。
3. **新系统开发角度**：新系统开发时只需对接统一的账号中心，简化开发流程。

#### 3. 核心功能
- 单点登录
- 单点登出
- 支持跨域单点登录
- 支持跨域单点登出

### 4.4.2 SSO系统设计与实现
#### 1. 核心应用与依赖
|  应用/模块/对象   |                  说明                   |
| :---------------: | :-------------------------------------: |
|   **前台站点**    |           需要**登录**的站点            |
|   SSO 站点-登录   |           提供**登录的页面**            |
|   SSO 站点-登出   |         提供**注销登录**的入口          |
|   SSO 服务-登录   |            提供**登录服务**             |
| SSO 服务-登录状态 | 提供**登录状态校验/登录信息查询**的服务 |
|   SSO 服务-登出   |       提供用户**注销登录**的服务        |
|      数据库       |          **存储用户账户信息**           |
|       缓存        | 存储用户的**登录信息，通常使用 Redis**  |

#### 2. 用户登录状态的存储与校验
常见的 Web 框架对于 Session 的实现都是生成一个 **SessionId** 存储在浏览器 **Cookie** 中。然后将 Session 内容存储在服务器端**内存**中。用户**登录成功**之后，生成 **AuthToken** 交给**客户端**保存。如果是浏览器，就保存在 **Cookie** 中。如果是移动端就保存在**本地缓存**中。这里探讨基于 Web 站点的 SSO。

用户在浏览**需要登录**的页面时，客户端将 **AuthToken** 提交给 **SSO 服务校验登录状态/获取用户登录信息**。对于登录信息的存储，建议采用 Redis，使用 Redis **集群** 来存储登录信息，既可以保证高可用，又可以拓展性。同时也可以让 SSO 服务满足负载均衡/可伸缩的需求。

|     对象      |                             说明                             |
| :-----------: | :----------------------------------------------------------: |
| **AuthToken** | 直接使用 UUID/GUID 即可，如果有验证 AuthToken 合法性需求，可以将 UserName +时间戳加密生成，服务端解密之后验证合法性 |
| **登录信息**  |            通常是将 UserId，UserName **缓存**起来            |

#### 3. 用户登录/登录校验
**登录时序图** 如下所示：
<img src="assets/sso-login-sequence.png-kbrb.png" alt="SSO系统设计-登录时序图" style="zoom:60%;" />

**登录信息获取/登录状态校验** 流程：
<img src="assets/sso-logincheck-sequence.png-kbrb.png" alt="SSO系统设计-登录信息获取/登录状态校验" style="zoom:60%;" />

#### 4. 用户登出
用户**登出**时要做的事情很简单：
1. 服务端**清除缓存（Redis）中的登录状态**。
2. 客户端**清除存储**的 AuthToken。

**登出时序图**
<img src="assets/sso-logout-sequence.png-kbrb.png" alt="SSO系统设计-用户登出" style="zoom:60%;" />

#### 5. 跨域登录/登出
**客户端存储 AuthToken**，服务器端通过 **Redis** 存储登录信息。由于客户端是将 AuthToken 存储在 Cookie 中的。所以**跨域要解决的**问题，就是如何解决 Cookie 的跨域读写问题。

**Cookie 是不能跨域的** ，比如一个解决跨域的核心思路就是：
- 登录完成之后通过**回调**的方式，将 AuthToken 传递给**主域名之外**的站点，该站点自行将 AuthToken 保存在**当前域下的 Cookie** 中。
- 登出完成之后通过**回调**的方式，调用**非主域名站点**的登出页面，完成设置 Cookie 中的 AuthToken 过期的操作。

## 4.5 分布式Session
> **集群部署时的分布式Session如何实现？**

分布式系统中最大的问题就是 **分布式事务**、**接口幂等性**、**分布式锁**，还有最后一个就是 **分布式 Session**。分布式系统下那么多的服务，Session 状态如何维护:
### 4.5.1 集群下的Session管理
一个用户的 Session 信息如果存储在一个**服务器**上，那么当负载均衡器把用户的下一个请求转发到另一个服务器，由于服务器没有用户的 Session 信息，那么该用户就需要重新进行登录等操作。
#### 1. Sticky Session
需要配置**负载均衡器**，使得**一个用户的所有请求都路由到同一个服务器**，这样就可以把用户的 Session 存放在该服务器中。

**缺点**：当服务器宕机时，将丢失该服务器上的所有 Session。
#### 2. Session复制
在服务器之间进行 **Session 同步** 操作，每个服务器都有 **所有用户的 Session 信** 息，因此用户可以向任何一个服务器进行请求。

缺点：**占用过多内存**；同步过程占用网络带宽以及服务器处理器时间。

#### 3. Session服务器
使用一个**单独的服务器存储 Session 数据**，可以使用传统的 MySQL，也使用 **Redis** 或者 Memcached 这种内存型数据库。

**优点**：为了使得大型网站具有伸缩性，集群中的应用服务器通常需要保持无状态，那么应用服务器不能存储用户的会话信息。Session Server 将用户的会话信息单独进行存储，从而保证了应用服务器的无状态。

**缺点**：需要去实现存取 Session 的代码。

### 4.5.2 Tomcat+Redis
这个方式挺方便，就是跟之前一样使用 Session 的代码，也是基于 Tomcat 原生的 Session 支持即可，区别就是使用 **Tomcat  RedisSessionManager**，让所有部署的 **Tomcat 都将 Session 数据存储到 Redis 即可**。

### 4.5.3 Spring Session+Redis
虽然 Tomcat + Redis 的方式好用，但是会**严重依赖于 Web 容器**，不好将代码移植到其他 Web 容器上去（比如 Jetty），尤其是更换技术栈就更难受了。给 Spring Session **配置基于 Redis 来存储 Session 数据**，然后配置了一个 **Spring Session 的过滤器**，这样的话，**Session 相关操作都会交给 Spring Session 了**。接着在代码中就可以使用原生的 Session 操作，就是**直接基于 Spring Session** 从 Redis 中获取数据了。

### 4.5.4 弃用Session
使用 **JWT Token** 储存用户身份，然后再从数据库或者 cache 中获取其他的信息。这样无论请求分配到哪个服务器都无所谓。

# 5. 网关
## 5.1 概述
### 5.1.1 什么是网关
**API 网关** 可以看做系统与外界联通的入口，可以在网关进行处理一些非业务逻辑的逻辑，比如权限验证，监控，缓存，请求路由等等。

一般情况下，网关一般都会提供**请求转发、安全认证（身份/权限认证）、流量控制、负载均衡、容灾、日志、监控**等功能。实际上网关主要做了一件事情：**请求过滤** 。

### 5.1.2 网关用处
微服务下一个系统被**拆分为多个服务**，但是像**安全认证，流量控制，日志，监控**等功能是**每个服务都需要**的，没有网关的话，就需要在每个服务中**单独实现**，这就会做很多重复的事情并且没有一个全局的视图来统一管理这些功能。

- **RPC 协议转成 HTTP**。由于在内部开发中都是以 RPC 协议(thrift or dubbo)去做开发，暴露给内部服务，当**外部服务**需要使用这个**接口**的时候往往需要将 RPC 协议转换成 HTTP 协议。
- **请求路由**：一个系统可能同一个接口新老两套系统都在使用，需要需要根据请求上下文将请求路由到对应的接口。
- **统一鉴权**：对于**鉴权操作**不涉及到业务逻辑，那么可以在网关层进行处理，不用放到到业务逻辑层面。
- **统一监控**：由于网关是外部服务的**入口**，所以可以通过网关获取如**入参出参，链路时间**等监控信息。
- **流量控制，熔断降级**：对于流量控制，熔断降级**非业务逻辑**可以统一放到网关层。

### 5.1.3 常见网关系统
经常用到的开源 API 网关系统有：
1. **Kong**
2. **Netflix zuul**

### 5.1.4 统一API网关
**统一的 API 网关** 不仅有 API 网关的所有的特点，还有下面几个好处：
1. **统一技术组件升级**：在公司中如果有某个技术组件需要升级，那么是需要和每个业务线沟通，通常几个月都搞不定。举个例子如果对于入口的安全鉴权有重大安全隐患需要升级，如果速度还是这么慢肯定是不行，那么有了统一的网关升级是很快的。
2. **统一服务接入**：对于某个服务的接入也比较困难，比如公司已经研发出了比较稳定的服务组件，正在公司大力推广，这个周期肯定也特别漫长，由于有了统一网关，那么只需要从网关统一接入。
3. **节约资源**：不同业务不同部门如果都自己搞一个网关层，那就太难受了。统一 API 网关可以节约资源。

## 5.2 统一网关的设计
### 5.2.1 异步化请求
如果是自己实现的网关层，由于只有自己使用，对于吞吐量的要求并不高所以，一般**同步请求调用**即可。

对于统一的网关层，如何用少量的机器接入更多的服务，这就需要**异步化**用来提高更多的吞吐量。对于异步化一般有下面两种策略：
- **Tomcat/Jetty+NIO+servlet3**：这种策略使用的比较普遍，京东，有赞，Zuul，都选取的是这个策略，这种策略比较**适合 HTTP**。在 Servlet3 中可以开启异步。
- **Netty+NIO**：Netty 为高并发而生，目前唯品会的网关使用这个策略，在唯品会的技术文章中在相同的情况下 Netty 是每秒 30w+ 的吞吐量，Tomcat 是 13w+，可以看出是有一定的差距的，但是 Netty 需要自己处理 HTTP 协议，这一块比较麻烦。

对于网关是 HTTP 请求场景比较多的情况可以**采用 Servlet**，毕竟有更加成熟的处理 HTTP 协议。如果更加**重视吞吐量**那么可以采用 **Netty**。

#### 1. 全链路异步
对于来的**请求已经使用异步**了，为了达到**全链路异步**所以需要对**去的请求**也进行异步处理，对于去的请求可以利用 **RPC 的异步支持** 进行异步请求，所以基本可以达到下图：
![](assets/166c877315e557af)

由在 web 容器中开启 **servlet 异步**，然后进入到**网关的业务线程池中进行业务处理**，然后进行 RPC 的**异步调用并注册需要回调的业务**，最后在回调线程池中进行回调处理。

#### 2. 链式处理
在设计模式中有一个模式叫**责任链模式**，其作用是避免请求发送者与接收者耦合在一起，让多个对象都有可能接收请求，将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。通过这种模式将请求的发送者和请求的处理者解耦了。各个框架中对此模式都有体现，比如 servlet 里面的 filter，Spring MVC 里面的 Interceptor。

可以借鉴这种设计模式到自己的网关设计中：
- **preFilters**：**前置过滤器**，用来处理一些公共的业务，比如**统一鉴权，统一限流，熔断降级，缓存处理**等，并且提供业务方扩展。
- **routingFilters**：用来处理一些**泛化调用**，主要是做**协议的转换，请求的路由**工作。
- **postFilters**：后置过滤器，主要用来做**结果的处理，日志打点，记录时间**等等。
- **errorFilters**：错误过滤器，用来**处理调用异常**的情况。

#### 3. 业务隔离
上面在**全链路异步**的情况下**不同业务之间的影响很小**，但是如果在提供的自定义 FiIlter 中进行了某些**同步调用**，一旦超时频繁那么就会对其他业务产生影响。所以**需要采用隔离之术，降低业务之间的互相影响**。

##### (1) 信号量隔离
信号量隔离只是**限制了总的并发数**，服务还是主线程进行**同步调用**。这个隔离如果远程调用超时依然会影响主线程，从而会影响其他业务。因此，如果只是想限制某个服务的总并发调用量或者调用的服务不涉及远程调用的话，可以使用**轻量级的信号量**来实现。有赞的网关由于没有自定义 filter 所以选取的是信号量隔离。

##### (2) 线程池隔离
最简单的就是**不同业务之间通过不同的线程池进行隔离**，就算业务接口出现了问题由于线程池已经进行了隔离那么也不会影响其他业务。使用一些其他语言比如 Golang 进行**开发网关**的话，线程是比较轻的资源，所以**比较适合使用线程池隔离**。

##### (3) 集群隔离
如果有某些业务就需要使用隔离但是统一网关又没有线程池隔离那么应该怎么办呢？那么可以使用**集群隔离**，如果某些业务真的很重要那么可以为这一系列业务单独申请一个集群或者多个集群，通过机器之间进行隔离。

#### 4. 请求限流
**流量控制** 可以采用很多开源的实现，比如阿里最近开源的 **Sentinel** 和比较成熟的 **Hystrix**。一般限流分为**集群限流**和**单机限流**:
- 利用**统一存储**保存**当前流量的情况**，一般可以采用 Redis，这个一般会有一些性能损耗。
- 单机限流：**限流每台机器** 可以直接利用 Guava 的 **令牌桶** 去做，由于没有远程调用所以性能消耗较小。

#### 5. 熔断降级
可以参照开源的实现 **Sentinel** 和 **Hystrix**。

#### 6. 泛化调用
**泛化调用** 指的是一些 **通信协议的转换**，比如将 HTTP 转换成 Thrift。在一些开源的网关中比如 Zuul 是没有实现的，因为各个公司的**内部服务通信协议都不同**。比如在唯品会中支持 HTTP1,HTTP2 以及二进制的协议，然后转化成**内部的协议**，淘宝的支持 HTTPS,HTTP1,HTTP2 这些协议都可以转换成，HTTP, HSF, **Dubbo** 等协议。
##### (1) 泛化调用
如何实现**泛化调用**？由于协议很难自动转换，那么其实**每个协议对应的接口需要提供一种映射**。简单来说就是**把两个协议都能转换成共同语言，从而互相转换**。

一般来说**共同语言**有三种方式指定：
- **JSON**：JSON 数据格式比较简单，解析速度快，较轻量级。
- **XML**：XML 数据比较重，解析比较困难，不过多讨论。
- **自定义描述语言**：一般来说这个成本比较高需要自己定义语言来进行描述并进行解析，但是其扩展性，自定义个性化性都是最高。

对于泛化调用如果要自己设计的话 **JSON 基本可以满足**，如果对于个性化的需要特别多的话倒是可以自己定义一套语言。

#### 7. 管理平台
上面介绍的都是**如何实现一个网关**的技术关键。这里需要介绍网关的一个业务关键。有了网关之后，需要一个管理平台去对上面所描述的技术关键进行**配置**，包括但不限于下面这些配置：限流、熔断、缓存、日志、自定义 filter、泛化调用等。

## 5.3 总结
一个合理的标准网关应该按照如下去实现：
![](assets/166c88416c4cc227)

# 6. 限流
## 6.1 基础
限流可以认为**服务降级**的一种，限流就是**限制系统的输入和输出流量已达到保护系统的目的**。一般来说系统的吞吐量是可以被测算的，为了保证系统的稳定运行，一旦达到的需要限制的阈值，就需要限制流量并采取一些措施以完成限制流量的目的。比如：**延迟处理，拒绝处理，或者部分拒绝处理** 等等。

限流可以应对：
- 热点业务带来的突发请求。
- 调用方 bug 导致的突发请求。
- 恶意攻击请求。

## 6.2 限流方法
实现限流有很多办法，在程序中时通常是根据**每秒处理的事务数** (Transactionpersecond) 来衡量接口的流量。

### 6.2.1 固定窗口计数器算法
#### 1. 概述
固定窗口计数器算法概念如下：
- 将时间划分为**多个窗口**。
- 在每个窗口内每有一次请求就将**计数器加一**。
- 如果计数器超过了限制数量，则本窗口内所有的请求都被丢弃，当时间到达下一个窗口时，计数器**重置**。

<img src="assets/image-20200726235529093.png" alt="image-20200726235529093" style="zoom: 50%;" />![image-20200726235704547](assets/image-20200726235704547.png)

固定窗口计数器是最为简单的算法，但这个算法有时会让通过请求量允许为限制的两倍。考虑如下情况：限制 1 秒内最多通过 5 个请求，在第一个窗口的**最后半秒内通过了 5 个请求**，第二个窗口的前半秒内又**通过了 5 个请求**。这样看来就是在 1 秒内通过了 **10 个请求**。
<img src="assets/image-20200726235727593.png" alt="image-20200726235727593" style="zoom:67%;" />

这种限流算法**无法保证限流速率**，因而无法保证突然激增的流量。比如我们限制一个接口一分钟只能访问 10 次的话，前半分钟一个请求没有接收，后半分钟接收了10个请求。

#### 2. 代码实现
- **控制单位时间内的请求数量**。
- 不足：**请求量是不知道的**。假设在 00:01 时发生一个请求，在 00:01-00:58 之间不发送请求，在 00:59 时发送**剩下的所有请求 n-1** (n 为限流请求数量)，再下一分钟的 00:01 发送 n 个请求，这样在 2 秒钟内请求到达了 2n - 1 个。

### 6.2.2 滑动窗口计数器算法
#### 1. 概述
滑动窗口是对计数器方式的改进, 增加一个**时间粒度的度量单位**。把一分钟分成若干等分(6份，每份 10 秒)，在每一份上设置独立计数器，在 00:00-00:09 之间发生请求计数器累加 1。当等分数量越大限流统计就越详细。

<img src="assets/image-20200726235916536.png" alt="image-20200726235916536" style="zoom:67%;" />

滑动窗口计数器算法概念如下：
- 将时间划分为**多个区间**。
- 在每个区间内每有一次请求就将计数器加一维持一个时间窗口，占据多个区间。
- 每经过一个区间的时间，则抛弃最老的一个区间，并纳入最新的一个区间。
- 如果当前窗口内区间的请求计数总和超过了限制数量，则本窗口内所有的请求都被丢弃。

滑动窗口计数器是通过将窗口再细分，并且**按照时间"滑动"** ，这种算法避免了固定窗口计数器带来的双倍突发请求，但时间区间的精度越高，算法所需的空间容量就越大。

### 6.2.3 漏桶算法
#### 1. 概述
漏桶算法概念如下：
- 将每个**请求**视作"**水滴**"放入"漏桶"进行存储。
- “漏桶"以**固定速率向外"漏"出请求来执行**，如果"漏桶"空了则停止"漏水”。
- 如果"漏桶"满了则多余的"水滴"会被**直接丢弃**。

漏桶算法多使用**队列实**现，服务的请求会存到队列中，服务的提供方则按照**固定的速率从队列中**取出请求并执行，过多的请求则放在队列中排队或直接拒绝。

<img src="assets/image-20200727000033941.png" alt="image-20200727000033941" style="zoom:67%;" />

漏桶算法的缺陷也很明显，当短时间内有**大量的突发请求**时，即便此时服务器没有任何负载，**每个请求也都得在队列中等待一段时间才能被响应**。

### 6.2.4 令牌桶算法
#### 1. 概述
规定固定容量的桶，**token 以固定速度往桶内填充**，当桶满时 token 不会被继续放入，每过来一个请求把 token 从桶中移除，如果桶中没有 token 则不能请求。

<img src="assets/image-20200727000232046.png" alt="image-20200727000232046" style="zoom:67%;" />

令牌桶算法概念如下：
- **令牌以固定速率生成**。
- 生成的令牌放入令牌桶**中存放**，如果令牌桶满了则多余的**令牌会直接丢弃**，当请求到达时，会尝试从令牌桶中**取令牌**，取到了令牌的请求可以执行。
- 如果桶空了，那么尝试取令牌的请求会被**直接丢弃**。

令牌桶算法既能够将所有的请求 **平均分布到时间区间内**，又能接受服务器能够承受范围内的突发请求，因此是目前使用 **较为广泛的一种限流算法**。

# 7. 测试
## 7.1 单元测试
- **白盒测试**：能看到 **完整代码** 的。
- **黑盒测试** ：**没有源码**，功能测试。
- **灰盒测试**：介于白盒测试与黑盒测试之间的一种测试，灰盒测试多用于**集成测试阶段**，不仅关注输出、输入的正确性，同时也关注程序内部的情况。

## 7.2 压力测试
并发数的问题，能承载多少并发。

## 7.3 疲劳强度测试
测试能否长期稳定运行。

## 7.4 冒烟测试
对主要流程测试，例如 支付环节。

## 7.5 集成测试
完整功能的测试，最终的是测试整体业务.

## 7.6 回归测试
增加一个功能。

## 7.7 自动化测试
编码、场景设计。
